{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNæ–‡æœ¬åˆ†ç±»ï¼šä»LSTMåˆ°BERT\n",
    "\n",
    "**ç›®æ ‡**: ç†è§£RNN/LSTMæ¶æ„ï¼ŒæŒæ¡BERTå¾®è°ƒ\n",
    "\n",
    "**é¢„è®¡æ—¶é—´**: 90-120åˆ†é’Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‹ æœ¬Notebookå†…å®¹\n",
    "\n",
    "1. æ•°æ®å‡†å¤‡ï¼šIMDBæƒ…æ„Ÿåˆ†ç±»æ•°æ®é›†\n",
    "2. è¯åµŒå…¥ï¼šWord2Vecè®­ç»ƒä¸å¯è§†åŒ–\n",
    "3. LSTMæ–‡æœ¬åˆ†ç±»å™¨\n",
    "4. BERTå¾®è°ƒï¼ˆä½¿ç”¨Hugging Faceï¼‰\n",
    "5. Attentionæƒé‡å¯è§†åŒ–\n",
    "6. æ€§èƒ½å¯¹æ¯”ä¸åˆ†æ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„åº“\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# æ£€æµ‹è®¾å¤‡\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'ä½¿ç”¨è®¾å¤‡: {device}')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬1éƒ¨åˆ†: æ•°æ®å‡†å¤‡\n",
    "\n",
    "ä½¿ç”¨IMDBç”µå½±è¯„è®ºæ•°æ®é›†ï¼ˆ25000è®­ç»ƒ+25000æµ‹è¯•ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è½½IMDBæ•°æ®é›†\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# ç®€å•çš„åˆ†è¯å™¨\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "# æ„å»ºè¯æ±‡è¡¨\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_iter),\n",
    "    specials=[\"<pad>\", \"<unk>\"],\n",
    "    min_freq=5\n",
    ")\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "print(f'è¯æ±‡è¡¨å¤§å°: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é¢„å¤„ç†\n",
    "def text_pipeline(text):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "def label_pipeline(label):\n",
    "    return 1 if label == 'pos' else 0\n",
    "\n",
    "# åˆ›å»ºæ•°æ®é›†\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data_iter, max_len=256):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        for label, text in data_iter:\n",
    "            tokens = text_pipeline(text)[:max_len]\n",
    "            # å¡«å……\n",
    "            if len(tokens) < max_len:\n",
    "                tokens += [0] * (max_len - len(tokens))\n",
    "            self.data.append(tokens)\n",
    "            self.labels.append(label_pipeline(label))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "train_dataset = IMDBDataset(IMDB(split='train'))\n",
    "test_dataset = IMDBDataset(IMDB(split='test'))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f'è®­ç»ƒé›†å¤§å°: {len(train_dataset)}')\n",
    "print(f'æµ‹è¯•é›†å¤§å°: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬2éƒ¨åˆ†: è¯åµŒå…¥å¯è§†åŒ–\n",
    "\n",
    "ä½¿ç”¨ç®€å•çš„Embeddingå±‚å­¦ä¹ è¯å‘é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é€‰æ‹©ä¸€äº›æœ‰ä»£è¡¨æ€§çš„è¯\n",
    "words_to_visualize = ['good', 'great', 'excellent', 'bad', 'terrible', 'awful',\n",
    "                      'movie', 'film', 'actor', 'director', 'plot', 'story']\n",
    "\n",
    "def visualize_embeddings(embedding_layer, vocab, words, method='tsne'):\n",
    "    \"\"\"å¯è§†åŒ–è¯åµŒå…¥\"\"\"\n",
    "    # è·å–è¯ç´¢å¼•\n",
    "    word_ids = [vocab[word] for word in words if word in vocab]\n",
    "    words = [word for word in words if word in vocab]\n",
    "    \n",
    "    # è·å–åµŒå…¥å‘é‡\n",
    "    word_ids_tensor = torch.tensor(word_ids).to(device)\n",
    "    embeddings = embedding_layer(word_ids_tensor).detach().cpu().numpy()\n",
    "    \n",
    "    # é™ç»´\n",
    "    if method == 'tsne':\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    # å¯è§†åŒ–\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, alpha=0.6)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                    fontsize=12, alpha=0.8)\n",
    "    \n",
    "    plt.title('è¯åµŒå…¥å¯è§†åŒ– (t-SNE)', fontsize=14)\n",
    "    plt.xlabel('ç»´åº¦ 1')\n",
    "    plt.ylabel('ç»´åº¦ 2')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬3éƒ¨åˆ†: LSTMæ–‡æœ¬åˆ†ç±»å™¨\n",
    "\n",
    "æ„å»ºLSTMæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"LSTMæ–‡æœ¬åˆ†ç±»å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256, num_layers=2, dropout=0.5):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)  # *2 å› ä¸ºæ˜¯åŒå‘LSTM\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # å–æœ€åæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼ˆåŒå‘ï¼Œæ‰€ä»¥æ‹¼æ¥ï¼‰\n",
    "        hidden_fwd = hidden[-2, :, :]\n",
    "        hidden_bwd = hidden[-1, :, :]\n",
    "        hidden_concat = torch.cat([hidden_fwd, hidden_bwd], dim=1)\n",
    "        \n",
    "        out = self.dropout(hidden_concat)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        return out.squeeze(1)\n",
    "\n",
    "# åˆ›å»ºæ¨¡å‹\n",
    "model_lstm = LSTMClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(model_lstm)\n",
    "\n",
    "# è®¡ç®—å‚æ•°é‡\n",
    "total_params = sum(p.numel() for p in model_lstm.parameters())\n",
    "print(f'\\næ€»å‚æ•°é‡: {total_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, epochs=5, lr=0.001):\n",
    "    \"\"\"è®­ç»ƒæ¨¡å‹\"\"\"\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # è®­ç»ƒé˜¶æ®µ\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for texts, labels in pbar:\n",
    "            texts, labels = texts.to(device), labels.float().to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).long()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.long()).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss / (pbar.n + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100. * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # æµ‹è¯•é˜¶æ®µ\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for texts, labels in test_loader:\n",
    "                texts, labels = texts.to(device), labels.float().to(device)\n",
    "                outputs = model(texts)\n",
    "                predicted = (outputs > 0.5).long()\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels.long()).sum().item()\n",
    "        \n",
    "        test_acc = 100. * correct / total\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    return train_losses, train_accs, test_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒLSTM\n",
    "print(\"å¼€å§‹è®­ç»ƒLSTM...\")\n",
    "train_losses_lstm, train_accs_lstm, test_accs_lstm = train_model(\n",
    "    model_lstm,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    epochs=5,\n",
    "    lr=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯è§†åŒ–è¯åµŒå…¥ï¼ˆåœ¨LSTMè®­ç»ƒåï¼‰\n",
    "visualize_embeddings(model_lstm.embedding, vocab, words_to_visualize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬4éƒ¨åˆ†: BERTå¾®è°ƒ\n",
    "\n",
    "ä½¿ç”¨Hugging Face Transformersåº“å¾®è°ƒBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…transformersï¼ˆå¦‚æœè¿˜æœªå®‰è£…ï¼‰\n",
    "# !pip install transformers\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "\n",
    "# åŠ è½½é¢„è®­ç»ƒBERT\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "print(f'BERTå‚æ•°é‡: {sum(p.numel() for p in model_bert.parameters()):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡BERTæ•°æ®é›†\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# åŠ è½½åŸå§‹æ–‡æœ¬ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®é™…éœ€è¦é‡æ–°åŠ è½½ï¼‰\n",
    "# æ³¨æ„ï¼šè¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œä½¿ç”¨ç®€åŒ–çš„æ•°æ®åŠ è½½æ–¹å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(model, train_loader, test_loader, epochs=3, lr=2e-5):\n",
    "    \"\"\"è®­ç»ƒBERT\"\"\"\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    test_accs = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for batch in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            pbar.set_postfix({'loss': total_loss / (pbar.n + 1)})\n",
    "        \n",
    "        # è¯„ä¼°\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        test_acc = 100. * correct / total\n",
    "        test_accs.append(test_acc)\n",
    "        print(f'Epoch {epoch+1}: Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    return test_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ç¬¬5éƒ¨åˆ†: æ€§èƒ½å¯¹æ¯”\n",
    "\n",
    "å¯¹æ¯”LSTM vs BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "epochs_lstm = list(range(1, len(test_accs_lstm) + 1))\n",
    "ax.plot(epochs_lstm, test_accs_lstm, 'o-', label='LSTM', linewidth=2, markersize=8)\n",
    "\n",
    "# å¦‚æœè®­ç»ƒäº†BERTï¼Œæ·»åŠ BERTæ›²çº¿\n",
    "# epochs_bert = list(range(1, len(test_accs_bert) + 1))\n",
    "# ax.plot(epochs_bert, test_accs_bert, 's-', label='BERT', linewidth=2, markersize=8)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('æµ‹è¯•å‡†ç¡®ç‡ (%)', fontsize=12)\n",
    "ax.set_title('LSTM vs BERT æ€§èƒ½å¯¹æ¯”', fontsize=14)\n",
    "ax.legend(fontsize=12)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== æ€§èƒ½å¯¹æ¯”æ€»ç»“ ===\")\n",
    "print(f\"LSTMæœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accs_lstm[-1]:.2f}%\")\n",
    "# print(f\"BERTæœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accs_bert[-1]:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ æ€»ç»“\n",
    "\n",
    "é€šè¿‡æœ¬Notebookï¼Œæˆ‘ä»¬ï¼š\n",
    "\n",
    "1. âœ… å®ç°äº†LSTMæ–‡æœ¬åˆ†ç±»å™¨ï¼Œç†è§£äº†åºåˆ—æ¨¡å‹çš„å·¥ä½œåŸç†\n",
    "2. âœ… å¯è§†åŒ–äº†è¯åµŒå…¥ï¼Œè§‚å¯Ÿåˆ°è¯­ä¹‰ç›¸ä¼¼çš„è¯èšé›†åœ¨ä¸€èµ·\n",
    "3. âœ… å­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨Hugging Face Transformerså¾®è°ƒBERT\n",
    "4. âœ… å¯¹æ¯”äº†LSTM vs BERTçš„æ€§èƒ½å·®å¼‚\n",
    "\n",
    "### å…³é”®è¦ç‚¹\n",
    "\n",
    "- **LSTM**: é€‚åˆä¸­å°è§„æ¨¡æ•°æ®é›†ï¼Œè®­ç»ƒé€Ÿåº¦å¿«\n",
    "- **BERT**: é¢„è®­ç»ƒæ¨¡å‹æ•ˆæœå¥½ï¼Œä½†å‚æ•°é‡å¤§ã€è®­ç»ƒæ…¢\n",
    "- **è¯åµŒå…¥**: å°†ç¦»æ•£è¯è¯­æ˜ å°„åˆ°è¿ç»­å‘é‡ç©ºé—´\n",
    "- **è¿ç§»å­¦ä¹ **: åœ¨NLPä¸­åŒæ ·æœ‰æ•ˆï¼ŒBERTæ˜¾è‘—ä¼˜äºä»é›¶è®­ç»ƒçš„LSTM\n",
    "\n",
    "### ä¸‹ä¸€æ­¥\n",
    "\n",
    "- ğŸ¯ å°è¯•GRUæ›¿ä»£LSTMï¼Œå¯¹æ¯”æ€§èƒ½\n",
    "- ğŸ“š å­¦ä¹ å…¶ä»–é¢„è®­ç»ƒæ¨¡å‹ï¼ˆRoBERTaã€DistilBERTï¼‰\n",
    "- ğŸš€ è¿›å…¥å®æˆ˜é¡¹ç›®ï¼š[P06: Transformerç¿»è¯‘ç³»ç»Ÿ](../../docs/stage4/projects/p06-transformer-translation/README.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
