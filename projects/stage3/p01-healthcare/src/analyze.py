#!/usr/bin/env python3
"""
P01 Healthcare Data Analysis - Main Analysis Script
åŒ»é™¢é”€å”®æ•°æ®åˆ†æä¸»è„šæœ¬

This script performs end-to-end analysis of hospital pharmaceutical sales data:
1. Data loading and validation
2. Data cleaning (missing values, duplicates, outliers)
3. Exploratory data analysis (EDA)
4. Visualization generation
5. Report creation

Usage:
    python src/analyze.py
    python src/analyze.py --config configs/custom.yaml
    python src/analyze.py --visualize-only
"""

import argparse
import logging
import sys
from pathlib import Path
from typing import Dict, Any, Tuple, Optional
import warnings

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import yaml

warnings.filterwarnings('ignore')


class HospitalSalesAnalyzer:
    """åŒ»é™¢é”€å”®æ•°æ®åˆ†æå™¨"""

    def __init__(self, config_path: str = "configs/default.yaml"):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self._setup_logging()
        self._setup_visualization()
        self.df: Optional[pd.DataFrame] = None
        self.df_clean: Optional[pd.DataFrame] = None
        self.results: Dict[str, Any] = {}

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = Path(config_path)
        if not config_file.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        log_config = self.config.get('logging', {})
        log_level = getattr(logging, log_config.get('level', 'INFO'))
        log_format = log_config.get('format', '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        log_file = log_config.get('file', 'outputs/analysis.log')

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        Path(log_file).parent.mkdir(parents=True, exist_ok=True)

        logging.basicConfig(
            level=log_level,
            format=log_format,
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)

    def _setup_visualization(self):
        """é…ç½®å¯è§†åŒ–æ ·å¼"""
        viz_config = self.config.get('visualization', {})
        figure_config = viz_config.get('figure', {})
        font_config = viz_config.get('fonts', {})

        # è®¾ç½®matplotlibæ ·å¼
        style = figure_config.get('style', 'seaborn-v0_8')
        try:
            plt.style.use(style)
        except:
            self.logger.warning(f"æ ·å¼ {style} ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æ ·å¼")

        # é…ç½®ä¸­æ–‡å­—ä½“
        font_family = font_config.get('family', ['SimHei', 'Arial Unicode MS', 'DejaVu Sans'])
        plt.rcParams['font.sans-serif'] = font_family
        plt.rcParams['axes.unicode_minus'] = False
        plt.rcParams['font.size'] = font_config.get('size', 12)

        # é…ç½®å›¾è¡¨é»˜è®¤å¤§å°
        self.default_figsize = figure_config.get('figsize_default', [12, 6])
        self.dpi = figure_config.get('dpi', 300)
        self.fig_format = figure_config.get('format', 'png')

        # é…ç½®é¢œè‰²
        color_config = viz_config.get('colors', {})
        self.palette = color_config.get('palette', 'Set2')
        self.cmap = color_config.get('cmap', 'YlOrRd')

    def load_data(self) -> pd.DataFrame:
        """åŠ è½½æ•°æ®"""
        self.logger.info("å¼€å§‹åŠ è½½æ•°æ®...")

        data_config = self.config.get('data', {})
        input_file = data_config.get('input_file', 'data/stage3/hospital_sales.csv')

        # è·å–åŠ è½½å‚æ•°
        loading_config = self.config.get('loading', {})
        encoding = loading_config.get('encoding', 'utf-8')
        parse_dates = loading_config.get('parse_dates', [])
        dtype = loading_config.get('dtype', {})

        # åŠ è½½æ•°æ®
        input_path = Path(input_file)
        if not input_path.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")

        self.df = pd.read_csv(
            input_path,
            encoding=encoding,
            parse_dates=parse_dates,
            dtype=dtype
        )

        self.logger.info(f"æ•°æ®åŠ è½½å®Œæˆ: {len(self.df)} è¡Œ, {len(self.df.columns)} åˆ—")
        self.logger.info(f"æ•°æ®å½¢çŠ¶: {self.df.shape}")

        return self.df

    def clean_data(self) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        self.logger.info("å¼€å§‹æ•°æ®æ¸…æ´—...")

        if self.df is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®")

        self.df_clean = self.df.copy()
        cleaning_config = self.config.get('cleaning', {})

        # 1. å¤„ç†ç¼ºå¤±å€¼
        self._handle_missing_values(cleaning_config.get('missing_values', {}))

        # 2. å¤„ç†é‡å¤å€¼
        self._handle_duplicates(cleaning_config.get('duplicates', {}))

        # 3. å¤„ç†å¼‚å¸¸å€¼
        self._handle_outliers(cleaning_config.get('outliers', {}))

        # 4. æå–æ—¥æœŸç‰¹å¾
        self._extract_date_features()

        self.logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: å‰©ä½™ {len(self.df_clean)} è¡Œ")

        return self.df_clean

    def _handle_missing_values(self, config: Dict[str, Any]):
        """å¤„ç†ç¼ºå¤±å€¼"""
        self.logger.info("å¤„ç†ç¼ºå¤±å€¼...")

        # ç»Ÿè®¡ç¼ºå¤±å€¼
        missing_counts = self.df_clean.isnull().sum()
        missing_pct = (missing_counts / len(self.df_clean) * 100).round(2)

        for col in missing_counts[missing_counts > 0].index:
            self.logger.info(f"  {col}: {missing_counts[col]} ({missing_pct[col]}%)")

        # å¡«å……ç‰¹å®šåˆ—
        for col, fill_value in config.items():
            if col in ['drop_columns', 'drop_rows_if_missing']:
                continue
            if col in self.df_clean.columns:
                before = self.df_clean[col].isnull().sum()
                self.df_clean[col].fillna(fill_value, inplace=True)
                self.logger.info(f"  å¡«å…… {col}: {before} ä¸ªç¼ºå¤±å€¼ â†’ '{fill_value}'")

        # åˆ é™¤å…³é”®åˆ—ç¼ºå¤±çš„è¡Œ
        drop_rows_if_missing = config.get('drop_rows_if_missing', [])
        if drop_rows_if_missing:
            before_len = len(self.df_clean)
            self.df_clean.dropna(subset=drop_rows_if_missing, inplace=True)
            dropped = before_len - len(self.df_clean)
            if dropped > 0:
                self.logger.info(f"  åˆ é™¤å…³é”®å­—æ®µç¼ºå¤±è¡Œ: {dropped} è¡Œ")

    def _handle_duplicates(self, config: Dict[str, Any]):
        """å¤„ç†é‡å¤å€¼"""
        self.logger.info("å¤„ç†é‡å¤å€¼...")

        subset = config.get('subset', None)
        keep = config.get('keep', 'first')

        duplicates = self.df_clean.duplicated(subset=subset)
        dup_count = duplicates.sum()

        if dup_count > 0:
            self.logger.info(f"  å‘ç° {dup_count} æ¡é‡å¤è®°å½•")
            self.df_clean.drop_duplicates(subset=subset, keep=keep, inplace=True)
            self.logger.info(f"  åˆ é™¤é‡å¤è®°å½•: ä¿ç•™ {keep}")
        else:
            self.logger.info("  æœªå‘ç°é‡å¤è®°å½•")

    def _handle_outliers(self, config: Dict[str, Any]):
        """å¤„ç†å¼‚å¸¸å€¼ï¼ˆIQRæ–¹æ³•ï¼‰"""
        if not config.get('enabled', False):
            self.logger.info("å¼‚å¸¸å€¼æ£€æµ‹å·²ç¦ç”¨")
            return

        self.logger.info("æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆIQRæ–¹æ³•ï¼‰...")

        columns = config.get('columns', [])
        iqr_multiplier = config.get('iqr_multiplier', 1.5)
        action = config.get('action', 'flag')

        outlier_info = {}

        for col in columns:
            if col not in self.df_clean.columns:
                continue

            Q1 = self.df_clean[col].quantile(0.25)
            Q3 = self.df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - iqr_multiplier * IQR
            upper_bound = Q3 + iqr_multiplier * IQR

            outliers = (self.df_clean[col] < lower_bound) | (self.df_clean[col] > upper_bound)
            outlier_count = outliers.sum()

            if outlier_count > 0:
                outlier_info[col] = {
                    'count': outlier_count,
                    'percentage': (outlier_count / len(self.df_clean) * 100).round(2),
                    'lower_bound': lower_bound,
                    'upper_bound': upper_bound
                }

                self.logger.info(f"  {col}: {outlier_count} ä¸ªå¼‚å¸¸å€¼ ({outlier_info[col]['percentage']}%)")
                self.logger.info(f"    èŒƒå›´: [{lower_bound:.2f}, {upper_bound:.2f}]")

                if action == 'remove':
                    self.df_clean = self.df_clean[~outliers]
                    self.logger.info(f"    å·²åˆ é™¤å¼‚å¸¸å€¼")
                elif action == 'cap':
                    self.df_clean.loc[self.df_clean[col] < lower_bound, col] = lower_bound
                    self.df_clean.loc[self.df_clean[col] > upper_bound, col] = upper_bound
                    self.logger.info(f"    å·²æˆªæ–­å¼‚å¸¸å€¼")
                elif action == 'flag':
                    self.df_clean[f'{col}_outlier'] = outliers
                    self.logger.info(f"    å·²æ ‡è®°å¼‚å¸¸å€¼ï¼ˆæ–°å¢åˆ— {col}_outlierï¼‰")

        self.results['outliers'] = outlier_info

    def _extract_date_features(self):
        """æå–æ—¥æœŸç‰¹å¾"""
        self.logger.info("æå–æ—¥æœŸç‰¹å¾...")

        if 'order_date' in self.df_clean.columns:
            self.df_clean['year'] = self.df_clean['order_date'].dt.year
            self.df_clean['month'] = self.df_clean['order_date'].dt.month
            self.df_clean['quarter'] = self.df_clean['order_date'].dt.quarter
            self.df_clean['day_of_week'] = self.df_clean['order_date'].dt.dayofweek
            self.df_clean['week_of_year'] = self.df_clean['order_date'].dt.isocalendar().week
            self.logger.info("  å·²æå–: year, month, quarter, day_of_week, week_of_year")

    def analyze(self) -> Dict[str, Any]:
        """æ‰§è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æ"""
        self.logger.info("å¼€å§‹æ¢ç´¢æ€§æ•°æ®åˆ†æ...")

        if self.df_clean is None:
            raise ValueError("è¯·å…ˆæ¸…æ´—æ•°æ®")

        analysis_config = self.config.get('analysis', {})

        # 1. æ€»ä½“æŒ‡æ ‡
        self._calculate_overall_metrics()

        # 2. æŒ‰ç»´åº¦åˆ†æ
        self._analyze_by_dimensions(analysis_config.get('dimensions', []))

        # 3. æ—¶é—´åºåˆ—åˆ†æ
        self._analyze_time_series(analysis_config.get('time_aggregations', []))

        # 4. TOP Nåˆ†æ
        self._analyze_top_n(analysis_config.get('top_n', 10))

        self.logger.info("æ¢ç´¢æ€§æ•°æ®åˆ†æå®Œæˆ")

        return self.results

    def _calculate_overall_metrics(self):
        """è®¡ç®—æ€»ä½“æŒ‡æ ‡"""
        self.logger.info("è®¡ç®—æ€»ä½“æŒ‡æ ‡...")

        metrics = {
            'total_sales': self.df_clean['total_amount'].sum(),
            'total_orders': len(self.df_clean),
            'average_order_value': self.df_clean['total_amount'].mean(),
            'average_unit_price': self.df_clean['unit_price'].mean(),
            'total_quantity': self.df_clean['quantity'].sum(),
            'date_range': {
                'start': str(self.df_clean['order_date'].min().date()),
                'end': str(self.df_clean['order_date'].max().date())
            }
        }

        self.results['overall_metrics'] = metrics

        self.logger.info(f"  æ€»é”€å”®é¢: {metrics['total_sales']:,.2f} å…ƒ")
        self.logger.info(f"  æ€»è®¢å•æ•°: {metrics['total_orders']:,}")
        self.logger.info(f"  å¹³å‡è®¢å•é‡‘é¢: {metrics['average_order_value']:.2f} å…ƒ")

    def _analyze_by_dimensions(self, dimensions: list):
        """æŒ‰ç»´åº¦åˆ†æ"""
        self.logger.info("æŒ‰ç»´åº¦åˆ†æ...")

        dimension_results = {}

        for dim in dimensions:
            if dim not in self.df_clean.columns:
                continue

            # æŒ‰ç»´åº¦æ±‡æ€»é”€å”®é¢
            dim_sales = self.df_clean.groupby(dim)['total_amount'].agg(['sum', 'count', 'mean'])
            dim_sales = dim_sales.sort_values('sum', ascending=False)
            dim_sales.columns = ['total_sales', 'order_count', 'avg_order_value']

            dimension_results[dim] = dim_sales

            self.logger.info(f"  {dim}: {len(dim_sales)} ä¸ªç±»åˆ«")

        self.results['dimensions'] = dimension_results

    def _analyze_time_series(self, aggregations: list):
        """æ—¶é—´åºåˆ—åˆ†æ"""
        self.logger.info("æ—¶é—´åºåˆ—åˆ†æ...")

        time_series_results = {}

        for agg in aggregations:
            if agg == 'daily':
                ts = self.df_clean.groupby('order_date')['total_amount'].sum()
            elif agg == 'monthly':
                ts = self.df_clean.groupby(self.df_clean['order_date'].dt.to_period('M'))['total_amount'].sum()
            elif agg == 'quarterly':
                ts = self.df_clean.groupby(self.df_clean['order_date'].dt.to_period('Q'))['total_amount'].sum()
            elif agg == 'yearly':
                ts = self.df_clean.groupby('year')['total_amount'].sum()
            else:
                continue

            time_series_results[agg] = ts
            self.logger.info(f"  {agg}: {len(ts)} ä¸ªæ—¶é—´ç‚¹")

        self.results['time_series'] = time_series_results

    def _analyze_top_n(self, n: int):
        """TOP Nåˆ†æ"""
        self.logger.info(f"TOP {n} åˆ†æ...")

        # TOP N äº§å“ï¼ˆæŒ‰é”€å”®é¢ï¼‰
        top_products = self.df_clean.groupby('product_name')['total_amount'].sum().sort_values(ascending=False).head(n)
        self.results['top_products'] = top_products
        self.logger.info(f"  TOP {n} äº§å“ï¼ˆé”€å”®é¢ï¼‰")

        # TOP N äº§å“ï¼ˆæŒ‰é”€é‡ï¼‰
        top_products_volume = self.df_clean.groupby('product_name')['quantity'].sum().sort_values(ascending=False).head(n)
        self.results['top_products_volume'] = top_products_volume
        self.logger.info(f"  TOP {n} äº§å“ï¼ˆé”€é‡ï¼‰")

    def visualize(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        self.logger.info("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        if self.df_clean is None:
            raise ValueError("è¯·å…ˆæ¸…æ´—æ•°æ®")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = Path(self.config.get('data', {}).get('output_dir', 'outputs'))
        figures_dir = output_dir / 'figures'
        figures_dir.mkdir(parents=True, exist_ok=True)

        charts_config = self.config.get('visualization', {}).get('charts', [])

        for chart in charts_config:
            if not chart.get('enabled', True):
                continue

            chart_name = chart.get('name')
            chart_type = chart.get('type')
            chart_title = chart.get('title')

            self.logger.info(f"  ç”Ÿæˆå›¾è¡¨: {chart_name} ({chart_type})")

            try:
                if chart_name == 'monthly_trend':
                    self._plot_monthly_trend(figures_dir, chart_title)
                elif chart_name == 'category_pie':
                    self._plot_category_pie(figures_dir, chart_title)
                elif chart_name == 'top10_products':
                    self._plot_top10_products(figures_dir, chart_title)
                elif chart_name == 'heatmap':
                    self._plot_heatmap(figures_dir, chart_title)
                elif chart_name == 'customer_type_bar':
                    self._plot_customer_type_bar(figures_dir, chart_title)
                elif chart_name == 'quarterly_boxplot':
                    self._plot_quarterly_boxplot(figures_dir, chart_title)
            except Exception as e:
                self.logger.error(f"    ç”Ÿæˆå›¾è¡¨å¤±è´¥: {e}")

        self.logger.info(f"å›¾è¡¨å·²ä¿å­˜åˆ°: {figures_dir}")

    def _plot_monthly_trend(self, output_dir: Path, title: str):
        """æœˆåº¦é”€å”®è¶‹åŠ¿å›¾"""
        monthly_sales = self.results['time_series']['monthly']

        plt.figure(figsize=self.default_figsize)
        monthly_sales.plot(kind='line', marker='o', linewidth=2, markersize=6)
        plt.title(title, fontsize=16, fontweight='bold')
        plt.xlabel('æœˆä»½', fontsize=14)
        plt.ylabel('é”€å”®é¢ï¼ˆå…ƒï¼‰', fontsize=14)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(output_dir / 'monthly_trend.png', dpi=self.dpi, bbox_inches='tight')
        plt.close()

    def _plot_category_pie(self, output_dir: Path, title: str):
        """ç±»åˆ«é”€å”®é¢å æ¯”é¥¼å›¾"""
        category_sales = self.results['dimensions']['category']['total_sales']

        plt.figure(figsize=[10, 8])
        plt.pie(category_sales, labels=category_sales.index, autopct='%1.1f%%', startangle=90)
        plt.title(title, fontsize=16, fontweight='bold')
        plt.tight_layout()
        plt.savefig(output_dir / 'category_pie.png', dpi=self.dpi, bbox_inches='tight')
        plt.close()

    def _plot_top10_products(self, output_dir: Path, title: str):
        """TOP10è¯å“æŸ±çŠ¶å›¾"""
        top10 = self.results['top_products']

        plt.figure(figsize=self.default_figsize)
        top10.plot(kind='barh', color=sns.color_palette(self.palette, n_colors=len(top10)))
        plt.title(title, fontsize=16, fontweight='bold')
        plt.xlabel('é”€å”®é¢ï¼ˆå…ƒï¼‰', fontsize=14)
        plt.ylabel('è¯å“åç§°', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'top10_products.png', dpi=self.dpi, bbox_inches='tight')
        plt.close()

    def _plot_heatmap(self, output_dir: Path, title: str):
        """æœˆåº¦-ç±»åˆ«é”€å”®çƒ­åŠ›å›¾"""
        pivot_table = self.df_clean.pivot_table(
            values='total_amount',
            index='month',
            columns='category',
            aggfunc='sum'
        )

        plt.figure(figsize=[14, 8])
        sns.heatmap(pivot_table, annot=True, fmt='.0f', cmap=self.cmap, linewidths=0.5)
        plt.title(title, fontsize=16, fontweight='bold')
        plt.xlabel('ç±»åˆ«', fontsize=14)
        plt.ylabel('æœˆä»½', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'heatmap.png', dpi=self.dpi, bbox_inches='tight')
        plt.close()

    def _plot_customer_type_bar(self, output_dir: Path, title: str):
        """å®¢æˆ·ç±»å‹é”€å”®å¯¹æ¯”"""
        customer_sales = self.results['dimensions']['customer_type']['total_sales']

        plt.figure(figsize=[10, 6])
        customer_sales.plot(kind='bar', color=sns.color_palette(self.palette, n_colors=len(customer_sales)))
        plt.title(title, fontsize=16, fontweight='bold')
        plt.xlabel('å®¢æˆ·ç±»å‹', fontsize=14)
        plt.ylabel('é”€å”®é¢ï¼ˆå…ƒï¼‰', fontsize=14)
        plt.xticks(rotation=0)
        plt.tight_layout()
        plt.savefig(output_dir / 'customer_type_bar.png', dpi=self.dpi, bbox_inches='tight')
        plt.close()

    def _plot_quarterly_boxplot(self, output_dir: Path, title: str):
        """å­£åº¦é”€å”®é¢åˆ†å¸ƒç®±çº¿å›¾"""
        plt.figure(figsize=self.default_figsize)
        self.df_clean.boxplot(column='total_amount', by='quarter', grid=False)
        plt.title(title, fontsize=16, fontweight='bold')
        plt.suptitle('')  # ç§»é™¤é»˜è®¤æ ‡é¢˜
        plt.xlabel('å­£åº¦', fontsize=14)
        plt.ylabel('é”€å”®é¢ï¼ˆå…ƒï¼‰', fontsize=14)
        plt.tight_layout()
        plt.savefig(output_dir / 'quarterly_boxplot.png', dpi=self.dpi, bbox_inches='tight')
        plt.close()

    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        self.logger.info("ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

        output_dir = Path(self.config.get('data', {}).get('output_dir', 'outputs'))
        reports_dir = output_dir / 'reports'
        reports_dir.mkdir(parents=True, exist_ok=True)

        report_config = self.config.get('report', {})
        report_format = report_config.get('format', 'markdown')

        if report_format == 'markdown':
            self._generate_markdown_report(reports_dir)

        self.logger.info(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {reports_dir}")

    def _generate_markdown_report(self, output_dir: Path):
        """ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š"""
        metrics = self.results.get('overall_metrics', {})
        dimensions = self.results.get('dimensions', {})
        top_products = self.results.get('top_products', pd.Series())

        # è·å–TOPç±»åˆ«
        category_sales = dimensions.get('category', pd.DataFrame())
        top_category = category_sales.index[0] if len(category_sales) > 0 else 'N/A'
        top_category_sales = category_sales.iloc[0]['total_sales'] if len(category_sales) > 0 else 0

        report = f"""# æœé˜³åŒ»é™¢é”€å”®æ•°æ®åˆ†ææŠ¥å‘Š

## ğŸ“Š æ•°æ®æ¦‚è§ˆ

- **åˆ†æå‘¨æœŸ**: {metrics.get('date_range', {}).get('start', 'N/A')} è‡³ {metrics.get('date_range', {}).get('end', 'N/A')}
- **è®¢å•æ€»æ•°**: {metrics.get('total_orders', 0):,}
- **æ€»é”€å”®é¢**: {metrics.get('total_sales', 0):,.2f} å…ƒ
- **å¹³å‡è®¢å•é‡‘é¢**: {metrics.get('average_order_value', 0):.2f} å…ƒ
- **å¹³å‡å•ä»·**: {metrics.get('average_unit_price', 0):.2f} å…ƒ
- **æ€»é”€å”®æ•°é‡**: {metrics.get('total_quantity', 0):,}

---

## ğŸ” ä¸»è¦å‘ç°

### 1. é”€å”®é¢æœ€é«˜çš„ç±»åˆ«
**{top_category}**: {top_category_sales:,.2f} å…ƒ

### 2. ç•…é”€è¯å“ TOP 3
"""

        for i, (product, sales) in enumerate(top_products.head(3).items(), 1):
            report += f"{i}. **{product}**: {sales:,.2f} å…ƒ\n"

        report += f"""
### 3. å®¢æˆ·ç±»å‹åˆ†å¸ƒ
"""

        if 'customer_type' in dimensions:
            for customer_type, row in dimensions['customer_type'].iterrows():
                report += f"- **{customer_type}**: {row['total_sales']:,.2f} å…ƒ ({row['order_count']:,} è®¢å•)\n"

        report += """
---

## ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨

æœ¬æ¬¡åˆ†æç”Ÿæˆäº†ä»¥ä¸‹å›¾è¡¨ï¼ˆä¿å­˜åœ¨ `outputs/figures/` ç›®å½•ï¼‰ï¼š

1. **monthly_trend.png** - æœˆåº¦é”€å”®è¶‹åŠ¿
2. **category_pie.png** - å„ç±»åˆ«é”€å”®é¢å æ¯”
3. **top10_products.png** - é”€å”®é¢TOP10è¯å“
4. **heatmap.png** - æœˆåº¦-ç±»åˆ«é”€å”®çƒ­åŠ›å›¾
5. **customer_type_bar.png** - å®¢æˆ·ç±»å‹é”€å”®å¯¹æ¯”
6. **quarterly_boxplot.png** - å­£åº¦é”€å”®é¢åˆ†å¸ƒ

---

## ğŸ’¡ å»ºè®®

### åº“å­˜ç®¡ç†
1. åŠ å¼ºç•…é”€è¯å“ï¼ˆTOP10ï¼‰çš„åº“å­˜ç®¡ç†ï¼Œé¿å…ç¼ºè´§
2. å…³æ³¨é”€å”®é¢å æ¯”ä½çš„ç±»åˆ«ï¼Œè€ƒè™‘ä¿ƒé”€æˆ–è°ƒæ•´é‡‡è´­ç­–ç•¥

### é”€å”®ç­–ç•¥
1. æ ¹æ®æœˆåº¦è¶‹åŠ¿ä¼˜åŒ–è¥é”€æ´»åŠ¨æ—¶é—´
2. é’ˆå¯¹ä¸åŒå®¢æˆ·ç±»å‹å®šåˆ¶å·®å¼‚åŒ–æœåŠ¡æ–¹æ¡ˆ

### è¿è¥ä¼˜åŒ–
1. åˆ†æå­£èŠ‚æ€§è¶‹åŠ¿ï¼Œæå‰å¤‡è´§
2. è¯†åˆ«å¼‚å¸¸è®¢å•ï¼Œä¼˜åŒ–é£æ§æœºåˆ¶

---

## ğŸ“ æ•°æ®è´¨é‡è¯´æ˜

"""

        if 'outliers' in self.results:
            report += "### å¼‚å¸¸å€¼æ£€æµ‹ç»“æœ\n\n"
            for col, info in self.results['outliers'].items():
                report += f"- **{col}**: æ£€æµ‹åˆ° {info['count']} ä¸ªå¼‚å¸¸å€¼ ({info['percentage']}%)\n"
                report += f"  - æ­£å¸¸èŒƒå›´: [{info['lower_bound']:.2f}, {info['upper_bound']:.2f}]\n"

        report += f"""
---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
**åˆ†æå·¥å…·**: py_ai_tutorial P01 Healthcare Analysis
**ç‰ˆæœ¬**: 1.0.0
"""

        # ä¿å­˜æŠ¥å‘Š
        report_path = output_dir / 'summary_report.md'
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report)

        self.logger.info(f"  MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

    def save_processed_data(self):
        """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
        self.logger.info("ä¿å­˜æ¸…æ´—åçš„æ•°æ®...")

        if self.df_clean is None:
            raise ValueError("è¯·å…ˆæ¸…æ´—æ•°æ®")

        output_dir = Path(self.config.get('data', {}).get('processed_data_dir', 'outputs/processed_data'))
        output_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸ºCSV
        csv_path = output_dir / 'cleaned_data.csv'
        self.df_clean.to_csv(csv_path, index=False, encoding='utf-8')
        self.logger.info(f"  CSVå·²ä¿å­˜: {csv_path}")

        # ä¿å­˜ä¸ºParquetï¼ˆæ›´é«˜æ•ˆï¼‰
        try:
            parquet_path = output_dir / 'cleaned_data.parquet'
            self.df_clean.to_parquet(parquet_path, index=False)
            self.logger.info(f"  Parquetå·²ä¿å­˜: {parquet_path}")
        except Exception as e:
            self.logger.warning(f"  Parquetä¿å­˜å¤±è´¥: {e}")


def run_analysis(config_path: str = "configs/default.yaml",
                 visualize_only: bool = False) -> HospitalSalesAnalyzer:
    """
    è¿è¡Œå®Œæ•´åˆ†ææµç¨‹

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        visualize_only: ä»…ç”Ÿæˆå¯è§†åŒ–ï¼ˆéœ€è¦å·²æœ‰æ¸…æ´—åçš„æ•°æ®ï¼‰

    Returns:
        åˆ†æå™¨å®ä¾‹
    """
    analyzer = HospitalSalesAnalyzer(config_path)

    if visualize_only:
        # ä»…å¯è§†åŒ–æ¨¡å¼ï¼šåŠ è½½å·²æ¸…æ´—çš„æ•°æ®
        analyzer.logger.info("ä»…å¯è§†åŒ–æ¨¡å¼ï¼šåŠ è½½å·²æ¸…æ´—æ•°æ®...")
        processed_data_dir = Path(analyzer.config.get('data', {}).get('processed_data_dir', 'outputs/processed_data'))
        cleaned_data_path = processed_data_dir / 'cleaned_data.parquet'

        if not cleaned_data_path.exists():
            cleaned_data_path = processed_data_dir / 'cleaned_data.csv'

        if cleaned_data_path.exists():
            if cleaned_data_path.suffix == '.parquet':
                analyzer.df_clean = pd.read_parquet(cleaned_data_path)
            else:
                analyzer.df_clean = pd.read_csv(cleaned_data_path)
            analyzer.analyze()
            analyzer.visualize()
        else:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¸…æ´—åçš„æ•°æ®: {processed_data_dir}")
    else:
        # å®Œæ•´åˆ†ææµç¨‹
        analyzer.load_data()
        analyzer.clean_data()
        analyzer.analyze()
        analyzer.visualize()
        analyzer.generate_report()
        analyzer.save_processed_data()

    return analyzer


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="P01 åŒ»é™¢é”€å”®æ•°æ®åˆ†æ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python src/analyze.py
  python src/analyze.py --config configs/custom.yaml
  python src/analyze.py --visualize-only
        """
    )

    parser.add_argument(
        '--config',
        type=str,
        default='configs/default.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: configs/default.yaml)'
    )

    parser.add_argument(
        '--visualize-only',
        action='store_true',
        help='ä»…ç”Ÿæˆå¯è§†åŒ–ï¼ˆéœ€è¦å·²æœ‰æ¸…æ´—åçš„æ•°æ®ï¼‰'
    )

    args = parser.parse_args()

    try:
        analyzer = run_analysis(
            config_path=args.config,
            visualize_only=args.visualize_only
        )

        print("\n" + "="*60)
        print("âœ… åˆ†æå®Œæˆï¼")
        print("="*60)

        if not args.visualize_only:
            metrics = analyzer.results.get('overall_metrics', {})
            print(f"\næ€»é”€å”®é¢: {metrics.get('total_sales', 0):,.2f} å…ƒ")
            print(f"æ€»è®¢å•æ•°: {metrics.get('total_orders', 0):,}")
            print(f"å¹³å‡è®¢å•é‡‘é¢: {metrics.get('average_order_value', 0):.2f} å…ƒ")

            print("\nğŸ“Š è¾“å‡ºæ–‡ä»¶:")
            output_dir = Path(analyzer.config.get('data', {}).get('output_dir', 'outputs'))
            print(f"  - å›¾è¡¨: {output_dir / 'figures'}/")
            print(f"  - æŠ¥å‘Š: {output_dir / 'reports'}/")
            print(f"  - æ¸…æ´—æ•°æ®: {analyzer.config.get('data', {}).get('processed_data_dir', 'outputs/processed_data')}/")

        return 0

    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    sys.exit(main())
