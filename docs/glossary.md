# AI术语表 (AI Glossary)

本术语表提供AI/ML领域常用术语的中英文对照和通俗解释,帮助你快速理解核心概念。

---

## A

### Activation Function | 激活函数
**定义**: 神经网络中用于引入非线性的函数。

**通俗解释**: 就像给神经元加个"开关",决定信号要传多强。常见的有ReLU(像闸门,负数全关闭)、Sigmoid(像平滑开关,输出0-1之间)。

**常见类型**:
- **ReLU**: max(0, x),负数变0,正数保持
- **Sigmoid**: 1/(1+e^-x),输出0-1,像概率
- **Tanh**: 输出-1到1,比Sigmoid更陡
- **Softmax**: 多分类输出概率分布

**例子**: 判断图片是猫是狗,Sigmoid输出0.8表示80%是猫。

---

### Adam Optimizer | Adam优化器
**定义**: Adaptive Moment Estimation,自适应矩估计优化算法。

**通俗解释**: 像个聪明的GPS导航,会根据历史路线调整步长,既快又稳。比SGD更智能,自动调学习率。

**优点**:
- 自动调整学习率
- 对不同参数用不同速度
- 收敛快、鲁棒性好

**超参数**:
- `lr=0.001`: 学习率
- `betas=(0.9, 0.999)`: 动量参数
- `eps=1e-8`: 数值稳定项

**使用场景**: 深度学习首选,适用于大多数任务。

---

### Attention Mechanism | 注意力机制
**定义**: 让模型"关注"输入中最重要的部分。

**通俗解释**: 就像你看书时会重点看某些段落一样,模型也会给重要信息更多"注意力权重"。Transformer的核心。

**类型**:
- **Self-Attention**: 句子内部词与词的关系
- **Cross-Attention**: 两个序列之间的关系
- **Multi-Head Attention**: 多个注意力头并行

**例子**: 翻译"我爱北京天安门"时,翻译"Beijing"时会重点看"北京"这个词。

---

## B

### Batch Normalization | 批归一化
**定义**: 对每个batch的数据进行归一化处理。

**通俗解释**: 像工厂流水线,每批产品都标准化处理,防止有的太大有的太小。加速训练,防止梯度消失。

**作用**:
- 加速收敛(2-3倍)
- 允许更大学习率
- 减少对初始化的敏感
- 有轻微正则化效果

**位置**: 通常放在卷积/全连接层之后,激活函数之前。

---

### Backpropagation | 反向传播
**定义**: 计算损失函数对每个参数梯度的算法。

**通俗解释**: 像考试后复盘错题,从最终错误(损失)倒推每一步哪里做错了,然后调整策略(参数)。

**过程**:
1. 前向传播: 输入→输出
2. 计算损失: 预测vs实际
3. 反向传播: 损失→梯度
4. 更新参数: 梯度下降

**公式**: ∂Loss/∂W = ∂Loss/∂y · ∂y/∂W (链式法则)

---

### Bias | 偏置
**定义**: 神经网络中的偏移项,y = Wx + b中的b。

**通俗解释**: 像直线方程y=kx+b中的b,调整函数的位置。没有bias,所有函数都过原点,限制太大。

**作用**: 增加模型灵活性,让决策边界不必过原点。

---

## C

### CNN (Convolutional Neural Network) | 卷积神经网络
**定义**: 专门处理图像数据的神经网络。

**通俗解释**: 像人眼看图片,先看边缘、纹理、局部特征,再组合成完整认知。用卷积核扫描图片提取特征。

**核心组件**:
- **卷积层**: 提取局部特征(边缘、纹理)
- **池化层**: 降采样,保留主要特征
- **全连接层**: 分类决策

**经典架构**: AlexNet, VGG, ResNet, EfficientNet

**应用**: 图像分类、目标检测、图像分割

---

### Cross-Entropy Loss | 交叉熵损失
**定义**: 衡量预测概率分布与真实分布差异的损失函数。

**通俗解释**: 像考试打分,预测越准损失越小。用于分类任务,输出是概率分布。

**公式**: Loss = -Σ y_true · log(y_pred)

**例子**:
- 真实标签: 猫[1, 0, 0]
- 预测概率: [0.8, 0.1, 0.1]
- 损失: -log(0.8) = 0.22 (小=好)

**vs MSE**: 交叉熵用于分类,MSE用于回归。

---

## D

### Dropout | 丢弃法
**定义**: 训练时随机丢弃一部分神经元。

**通俗解释**: 像特种兵训练,随机让一些人"缺席",逼团队每个人都能独当一面。防止过拟合。

**工作原理**:
- 训练: 每个神经元以概率p被丢弃
- 测试: 使用所有神经元,输出乘(1-p)

**典型值**: p=0.5 (丢弃50%)

**优点**: 强力正则化,减少神经元依赖。

---

## E

### Embedding | 嵌入
**定义**: 将离散符号(词、ID)映射到连续向量空间。

**通俗解释**: 把文字变成数字向量,相似的词向量距离近。"国王-男人+女人≈女王"就是embedding的魔力。

**例子**:
- 词: "猫" → 向量: [0.2, -0.5, 0.8, ...]
- 词: "狗" → 向量: [0.3, -0.4, 0.7, ...] (很接近)

**应用**:
- **Word2Vec**: 词嵌入
- **Image Embedding**: 图像特征向量
- **User Embedding**: 推荐系统用户表示

---

### Epoch | 训练轮次
**定义**: 全部训练数据过一遍模型叫一个epoch。

**通俗解释**: 像读书,一个epoch就是把课本完整看一遍。通常需要多个epoch(10-100)才能学好。

**vs Iteration**:
- 1 epoch = 所有数据
- 1 iteration = 1个batch

**例子**: 10000张图片,batch_size=100
- 1 epoch = 100 iterations
- 训练10 epochs = 1000 iterations

---

## F

### Fine-tuning | 微调
**定义**: 在预训练模型基础上,用少量数据调整参数。

**通俗解释**: 像请大厨(预训练模型)到你家,稍微调整菜谱(微调)就能做出定制菜(你的任务)。不用从头学厨艺。

**步骤**:
1. 加载预训练模型(如BERT)
2. 替换最后一层(任务相关)
3. 用小数据集训练
4. 学习率设很小(1e-5)

**优点**: 少量数据就能达到好效果。

---

## G

### Gradient Descent | 梯度下降
**定义**: 优化算法,沿梯度反方向更新参数。

**通俗解释**: 像下山,沿着最陡的方向走,最快到山谷(最小值)。

**公式**: W_new = W_old - lr · ∂Loss/∂W

**变种**:
- **SGD**: 每次用1个样本更新
- **Mini-batch GD**: 每次用一小批样本(常用)
- **Batch GD**: 每次用所有样本(慢)

**学习率**: 步长,太大震荡,太小慢。

---

## H

### Hyperparameter | 超参数
**定义**: 训练前人工设定的参数,不是学出来的。

**通俗解释**: 像游戏难度设置,你调的是"超参数"(难度级别),游戏里角色属性是"参数"(经验值)。

**常见超参数**:
- **学习率** (lr): 0.001, 0.01
- **Batch Size**: 32, 64, 128
- **Epochs**: 10, 50, 100
- **隐藏层数**: 2, 3, 5
- **Dropout率**: 0.5

**调优方法**: Grid Search, Random Search, Bayesian Optimization

---

## L

### Learning Rate | 学习率
**定义**: 梯度下降时的步长,控制参数更新幅度。

**通俗解释**: 下山的步子迈多大。太大容易跨过山谷(震荡),太小走得慢。

**典型值**:
- **Adam**: 0.001 (1e-3)
- **SGD**: 0.01-0.1
- **Fine-tuning**: 1e-5, 2e-5

**学习率衰减**: 开始大步走,后期小步精调。

**调度策略**:
- **Step Decay**: 每N个epoch减半
- **Cosine Annealing**: 余弦曲线衰减
- **Warm-up**: 前期慢慢增大

---

### Loss Function | 损失函数
**定义**: 衡量预测值与真实值差距的函数。

**通俗解释**: 像考试成绩,告诉模型"你错了多少"。训练目标就是最小化损失。

**常见类型**:
- **MSE** (均方误差): 回归任务
- **Cross-Entropy**: 分类任务
- **Hinge Loss**: SVM
- **Huber Loss**: 鲁棒回归

**例子**: 预测房价1000万,实际800万,MSE=(1000-800)²=40000

---

## M

### MPS (Metal Performance Shaders) | Metal性能着色器
**定义**: Apple芯片的GPU加速框架。

**通俗解释**: 苹果M系列芯片的"涡轮增压",让PyTorch训练快3-5倍。

**使用**:
```python
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
model.to(device)
```

**vs CUDA**:
- CUDA: NVIDIA GPU
- MPS: Apple GPU

---

## N

### NLP (Natural Language Processing) | 自然语言处理
**定义**: 让计算机理解和生成人类语言。

**通俗解释**: 教电脑"说人话",像Siri、ChatGPT能聊天、翻译、写文章。

**核心任务**:
- **分类**: 情感分析、垃圾邮件识别
- **序列标注**: 命名实体识别(NER)
- **生成**: 机器翻译、文本摘要
- **问答**: 智能客服、搜索引擎

**关键技术**: Tokenization, Embedding, Transformer, BERT, GPT

---

## O

### Overfitting | 过拟合
**定义**: 模型在训练集上表现好,但测试集差。

**通俗解释**: 像死记硬背考题,考原题100分,换个题型就不会。模型记住了训练数据细节,没学到通用规律。

**症状**:
- 训练loss↓↓, 验证loss↑↑
- 训练准确率95%, 测试准确率60%

**解决方法**:
- 增加数据
- Dropout / L2正则化
- 数据增强
- Early Stopping
- 简化模型

---

## P

### Pooling | 池化
**定义**: 降采样操作,减少特征图尺寸。

**通俗解释**: 像压缩图片,保留主要特征扔掉细节。减少计算量,增加感受野。

**类型**:
- **Max Pooling**: 取最大值(常用)
- **Average Pooling**: 取平均值
- **Global Pooling**: 整张图一个值

**例子**: 2x2 Max Pooling
```
[1 2]  →  [4]  (取4个数最大)
[3 4]
```

---

## R

### ReLU (Rectified Linear Unit) | 线性整流单元
**定义**: f(x) = max(0, x),负数变0,正数保持。

**通俗解释**: 像单向阀门,只让正值通过。简单高效,深度学习最常用激活函数。

**优点**:
- 计算快(一个max操作)
- 缓解梯度消失
- 稀疏激活(一半神经元是0)

**缺点**:
- **Dead ReLU**: 负值区域梯度永远0,神经元"死亡"

**变种**:
- **Leaky ReLU**: f(x) = max(0.01x, x)
- **PReLU**: 斜率可学习
- **GeLU**: GPT/BERT用这个

---

### RNN (Recurrent Neural Network) | 循环神经网络
**定义**: 处理序列数据,有"记忆"能力的神经网络。

**通俗解释**: 像看连续剧,前面的剧情会影响理解后面的。处理文本、时间序列时,前面的词/时刻会影响后面。

**问题**:
- **梯度消失**: 长序列记不住前面
- **梯度爆炸**: 梯度指数增长

**改进版**:
- **LSTM**: 长短期记忆网络,解决梯度消失
- **GRU**: 简化版LSTM,更快

**vs Transformer**: RNN串行慢,Transformer并行快,现在NLP多用Transformer。

---

## S

### SGD (Stochastic Gradient Descent) | 随机梯度下降
**定义**: 每次用一个或一小批样本更新参数。

**通俗解释**: 不用看完整本书(全部数据)才复习,边看边总结。快但有波动。

**优点**: 快,适合大数据
**缺点**: 震荡大,需调学习率

**vs Adam**:
- SGD: 固定学习率,简单
- Adam: 自适应学习率,智能

---

### Softmax | 软最大值函数
**定义**: 将向量归一化为概率分布。

**通俗解释**: 把一堆分数变成概率(和为1),最大的概率也最大。多分类的最后一层。

**公式**: softmax(x_i) = e^x_i / Σ e^x_j

**例子**:
- 输入: [2.0, 1.0, 0.1]
- 输出: [0.659, 0.242, 0.099] (和=1)

**应用**: 多分类任务输出层。

---

## T

### Transformer
**定义**: 基于自注意力机制的神经网络架构。

**通俗解释**: NLP革命性架构,让ChatGPT、BERT成为可能。用注意力取代RNN,训练快、效果好。

**核心组件**:
- **Self-Attention**: 词与词关系
- **Multi-Head Attention**: 多个注意力头
- **Position Encoding**: 位置信息
- **Feed-Forward**: 前馈网络

**优点**:
- 并行计算(vs RNN串行)
- 长距离依赖
- 可解释性(注意力权重)

**应用**: BERT, GPT, T5, Vision Transformer

---

### Transfer Learning | 迁移学习
**定义**: 将一个任务学到的知识迁移到另一个任务。

**通俗解释**: 像学会骑自行车后,学摩托车就快了。用大数据集预训练的模型,微调到你的小任务。

**流程**:
1. **预训练**: ImageNet预训练(1400万图片)
2. **迁移**: 用于猫狗分类(1000张图片)
3. **微调**: 只调最后几层

**优点**: 少量数据达到好效果。

---

## U

### Underfitting | 欠拟合
**定义**: 模型太简单,训练集和测试集都表现差。

**通俗解释**: 像学渣,考题和作业都不会。模型太简单,没学到数据规律。

**症状**:
- 训练loss高,验证loss高
- 两者都下降慢

**解决方法**:
- 增加模型复杂度(更多层/神经元)
- 训练更多epoch
- 减少正则化
- 增加特征

---

## V

### Validation Set | 验证集
**定义**: 用于调超参数和模型选择的数据集。

**通俗解释**: 像模拟考,帮你调策略但不算入最终成绩(测试集)。

**数据分割**:
- **训练集** 70%: 训练模型
- **验证集** 15%: 调超参数
- **测试集** 15%: 最终评估

**vs测试集**:
- 验证集: 开发阶段看,可调整
- 测试集: 最后才看,不能调

---

## W

### Weight | 权重
**定义**: 神经网络中连接的强度,y = Wx + b中的W。

**通俗解释**: 像食谱配比,盐放多少、糖放多少。训练就是调配比(权重)让菜(输出)好吃。

**初始化**:
- **Xavier**: 适合Sigmoid/Tanh
- **He**: 适合ReLU
- **随机**: 打破对称性

**更新**: 梯度下降 W = W - lr · ∂Loss/∂W

---

## 附录: 快速查询表

| 中文 | 英文 | 简写 | 适用场景 |
|------|------|------|----------|
| 机器学习 | Machine Learning | ML | 泛指 |
| 深度学习 | Deep Learning | DL | 神经网络 |
| 卷积神经网络 | Convolutional NN | CNN | 图像 |
| 循环神经网络 | Recurrent NN | RNN | 序列 |
| 长短期记忆 | Long Short-Term Memory | LSTM | 长序列 |
| 自然语言处理 | Natural Language Processing | NLP | 文本 |
| 生成对抗网络 | Generative Adversarial Network | GAN | 生成 |
| 强化学习 | Reinforcement Learning | RL | 决策 |
| 迁移学习 | Transfer Learning | - | 少样本 |
| 对抗训练 | Adversarial Training | - | 鲁棒性 |

---

**学习建议**:
1. 不用一次全记住,遇到不懂的词再查
2. 重点理解"通俗解释",公式可后面慢慢看
3. 边做项目边查术语,用中学最快
4. 记住英文缩写(CNN/RNN/NLP),看论文会用到

**继续学习**:
- 深入原理: 看每个阶段的理论文档
- 实践应用: 做项目时对照术语表
- 扩展阅读: 《深度学习》(花书) 第2-6章
