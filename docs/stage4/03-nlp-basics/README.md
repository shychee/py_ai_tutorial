# æ¨¡å—M03: è‡ªç„¶è¯­è¨€å¤„ç†åŸºç¡€

**é˜¶æ®µ**: Stage 4 - æ·±åº¦å­¦ä¹ 
**é¢„è®¡å­¦ä¹ æ—¶é—´**: 3-4å°æ—¶ï¼ˆç†è®ºï¼‰+ 3-4å°æ—¶ï¼ˆå®è·µï¼‰
**éš¾åº¦**: â­â­â­â­ ä¸­é«˜ç­‰

---

## ğŸ“š å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å°†èƒ½å¤Ÿï¼š

- âœ… ç†è§£è¯åµŒå…¥æŠ€æœ¯ï¼ˆWord2Vecã€GloVeã€FastTextï¼‰çš„åŸç†ä¸åº”ç”¨
- âœ… æŒæ¡å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNã€LSTMã€GRUï¼‰å¤„ç†åºåˆ—æ•°æ®çš„æœºåˆ¶
- âœ… æ·±å…¥ç†è§£Transformeræ¶æ„ï¼ˆSelf-Attentionã€Multi-Head Attentionï¼‰
- âœ… ç†Ÿæ‚‰é¢„è®­ç»ƒæ¨¡å‹ï¼ˆBERTã€GPTã€T5ï¼‰çš„åŸç†ä¸å¾®è°ƒæ–¹æ³•
- âœ… èƒ½å¤Ÿå®Œæˆæ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ç­‰NLPä»»åŠ¡
- âœ… æŒæ¡ä½¿ç”¨Hugging Face Transformersåº“è¿›è¡Œæ¨¡å‹å¾®è°ƒ

---

## ğŸ¯ æ ¸å¿ƒçŸ¥è¯†ç‚¹

### 1. è¯åµŒå…¥ (Word Embeddings)

#### 1.1 ä¸ºä»€ä¹ˆéœ€è¦è¯åµŒå…¥ï¼Ÿ

**ä¼ ç»Ÿè¡¨ç¤ºæ–¹æ³•çš„é—®é¢˜**ï¼š

**One-Hotç¼–ç **:
```python
vocab = ["king", "queen", "man", "woman", "apple"]
"king"  = [1, 0, 0, 0, 0]
"queen" = [0, 1, 0, 0, 0]
```

**ç¼ºç‚¹**ï¼š
- ç»´åº¦ç¾éš¾ï¼ˆè¯æ±‡é‡10ä¸‡ â†’ 10ä¸‡ç»´å‘é‡ï¼‰
- æ— æ³•è¡¨ç¤ºè¯è¯­ä¹‹é—´çš„è¯­ä¹‰å…³ç³»
- è¯å‘é‡æ­£äº¤ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦=0ï¼‰

**è¯åµŒå…¥çš„ä¼˜åŠ¿**:
```python
# è¯åµŒå…¥å°†è¯æ˜ å°„åˆ°ä½ç»´ç¨ å¯†å‘é‡ï¼ˆå¦‚300ç»´ï¼‰
"king"  = [0.50, 0.33, ..., -0.21]  # 300ç»´
"queen" = [0.48, 0.31, ..., -0.19]  # è¯­ä¹‰ç›¸è¿‘

# å¯ä»¥è¿›è¡Œå‘é‡è¿ç®—
vec("king") - vec("man") + vec("woman") â‰ˆ vec("queen")
```

#### 1.2 Word2Vec

**ä¸¤ç§è®­ç»ƒæ–¹å¼**ï¼š

**1) CBOW (Continuous Bag of Words)**:
```
ä¸Šä¸‹æ–‡: [the, cat, on, the] â†’ é¢„æµ‹ä¸­å¿ƒè¯: "sat"
```

**2) Skip-gram**:
```
ä¸­å¿ƒè¯: "sat" â†’ é¢„æµ‹ä¸Šä¸‹æ–‡: [the, cat, on, the]
```

**ç½‘ç»œç»“æ„**ï¼ˆSkip-gramï¼‰:
```
è¾“å…¥å±‚(one-hot) â†’ éšè—å±‚(embedding) â†’ è¾“å‡ºå±‚(softmax)
    10000ç»´           300ç»´             10000ç»´
```

**è®­ç»ƒæŠ€å·§**ï¼š
- **è´Ÿé‡‡æ · (Negative Sampling)**: ä¸è®¡ç®—æ‰€æœ‰10000ä¸ªè¯çš„softmaxï¼Œåªè®¡ç®—1ä¸ªæ­£æ ·æœ¬+kä¸ªè´Ÿæ ·æœ¬ï¼ˆk=5-20ï¼‰
- **å±‚æ¬¡Softmax (Hierarchical Softmax)**: ä½¿ç”¨äºŒå‰æ ‘ç»“æ„ï¼Œå¤æ‚åº¦ä»O(V)é™åˆ°O(log V)

**ä»£ç ç¤ºä¾‹** (ä½¿ç”¨Gensim):
```python
from gensim.models import Word2Vec

# è®­ç»ƒWord2Vecæ¨¡å‹
sentences = [["I", "love", "NLP"], ["Deep", "learning", "is", "fun"]]
model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)

# æŸ¥è¯¢ç›¸ä¼¼è¯
similar_words = model.wv.most_similar("love", topn=5)
print(similar_words)

# å‘é‡è¿ç®—
result = model.wv.most_similar(
    positive=['woman', 'king'],
    negative=['man'],
    topn=1
)
print(result)  # è¾“å‡º: [('queen', 0.87)]
```

#### 1.3 GloVe (Global Vectors)

**æ ¸å¿ƒæ€æƒ³**: ç»“åˆå…¨å±€ç»Ÿè®¡ä¿¡æ¯ï¼ˆè¯å…±ç°çŸ©é˜µï¼‰ä¸å±€éƒ¨ä¸Šä¸‹æ–‡çª—å£ã€‚

**è®­ç»ƒç›®æ ‡**:
```
J = Î£ f(X_ij) Â· (w_i^T Â· w_j + b_i + b_j - log(X_ij))Â²
```
å…¶ä¸­ï¼š
- `X_ij`: è¯iå’Œè¯jçš„å…±ç°æ¬¡æ•°
- `w_i, w_j`: è¯å‘é‡
- `f(X_ij)`: æƒé‡å‡½æ•°ï¼ˆå‡å°‘é«˜é¢‘è¯å½±å“ï¼‰

**GloVe vs Word2Vec**:
| ç‰¹æ€§ | Word2Vec | GloVe |
|------|----------|-------|
| è®­ç»ƒæ–¹å¼ | å±€éƒ¨ä¸Šä¸‹æ–‡çª—å£ | å…¨å±€å…±ç°çŸ©é˜µ |
| è®­ç»ƒé€Ÿåº¦ | è¾ƒå¿« | è¾ƒæ…¢ï¼ˆéœ€ç»Ÿè®¡å…±ç°çŸ©é˜µï¼‰ |
| æ€§èƒ½ | ç•¥ä½ | ç•¥é«˜ |
| é€‚ç”¨åœºæ™¯ | å¤§è§„æ¨¡è¯­æ–™ | ä¸­å°è§„æ¨¡è¯­æ–™ |

#### 1.4 FastText

**æ ¸å¿ƒåˆ›æ–°**: è€ƒè™‘**å­è¯ä¿¡æ¯ (Subword Information)**

**ç¤ºä¾‹**:
```
Word2Vec: "apple" â†’ [0.2, 0.3, ...]
FastText:  "apple" â†’ <ap, app, ppl, ple, le> çš„å¹³å‡
```

**ä¼˜åŠ¿**:
- å¤„ç†**æœªç™»å½•è¯ (OOV, Out-of-Vocabulary)**:
  ```python
  # Word2Vecæ— æ³•å¤„ç†
  "apples" (æœªè§è¿‡) â†’ âŒ æ— å‘é‡

  # FastTextå¯ä»¥ç»„åˆå­è¯
  "apples" â†’ <ap, app, ppl, ple, les, es> â†’ âœ… æœ‰å‘é‡
  ```
- å¯¹**å½¢æ€ä¸°å¯Œçš„è¯­è¨€**ï¼ˆå¦‚å¾·è¯­ã€åœŸè€³å…¶è¯­ï¼‰æ•ˆæœæ›´å¥½

**ä»£ç ç¤ºä¾‹**:
```python
from gensim.models import FastText

model = FastText(sentences, vector_size=100, window=5, min_count=1)

# å¤„ç†æœªç™»å½•è¯
oov_vector = model.wv['unknownword']  # å¯ä»¥ç”Ÿæˆå‘é‡
```

---

### 2. å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)

#### 2.1 ä¸ºä»€ä¹ˆéœ€è¦RNNï¼Ÿ

**é—®é¢˜**: ä¼ ç»Ÿç¥ç»ç½‘ç»œæ— æ³•å¤„ç†å˜é•¿åºåˆ—ã€‚

**RNNçš„ä¼˜åŠ¿**:
- å…±äº«å‚æ•°ï¼ˆä¸åŒæ—¶é—´æ­¥ä½¿ç”¨ç›¸åŒæƒé‡ï¼‰
- ä¿æŒå†å²ä¿¡æ¯ï¼ˆéšè—çŠ¶æ€è®°å¿†ï¼‰
- å¯å¤„ç†ä»»æ„é•¿åº¦åºåˆ—

**RNNç»“æ„**:
```
     y_t (è¾“å‡º)
      â†‘
     h_t (éšè—çŠ¶æ€)
    â†—  â†–
  h_{t-1}  x_t (è¾“å…¥)
```

**æ•°å­¦å…¬å¼**:
```
h_t = tanh(W_hh Â· h_{t-1} + W_xh Â· x_t + b_h)
y_t = W_hy Â· h_t + b_y
```

**ä»£ç ç¤ºä¾‹** (PyTorch):
```python
import torch.nn as nn

class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # x: (batch, seq_len, input_size)
        out, hidden = self.rnn(x)
        # out: (batch, seq_len, hidden_size)
        out = self.fc(out[:, -1, :])  # å–æœ€åæ—¶é—´æ­¥
        return out
```

#### 2.2 RNNçš„é—®é¢˜ï¼šæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

**æ¢¯åº¦æ¶ˆå¤±**:
```
âˆ‚L/âˆ‚h_1 = âˆ‚L/âˆ‚h_T Â· âˆ‚h_T/âˆ‚h_{T-1} Â· ... Â· âˆ‚h_2/âˆ‚h_1
         = âˆ‚L/âˆ‚h_T Â· W^{T-1}
```

å¦‚æœ `W < 1`ï¼Œè¿ä¹˜Tæ¬¡åæ¢¯åº¦â†’0ï¼ˆé•¿æœŸä¾èµ–æ¶ˆå¤±ï¼‰
å¦‚æœ `W > 1`ï¼Œè¿ä¹˜Tæ¬¡åæ¢¯åº¦â†’âˆï¼ˆæ¢¯åº¦çˆ†ç‚¸ï¼‰

**è§£å†³æ–¹æ¡ˆ**:
- **æ¢¯åº¦è£å‰ª (Gradient Clipping)**: é™åˆ¶æ¢¯åº¦æœ€å¤§å€¼
- **æ›´å¥½çš„æ¿€æ´»å‡½æ•°**: ReLUæ›¿ä»£tanh
- **é—¨æ§æœºåˆ¶**: LSTMã€GRU

#### 2.3 LSTM (Long Short-Term Memory)

**æ ¸å¿ƒæ€æƒ³**: å¼•å…¥**è®°å¿†ç»†èƒ (Cell State)** å’Œ**ä¸‰ä¸ªé—¨æ§å•å…ƒ**ã€‚

**LSTMç»“æ„**:
```
    è¾“å…¥é—¨     é—å¿˜é—¨     è¾“å‡ºé—¨
     i_t       f_t       o_t
      â†“         â†“         â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Cell State (C_t)     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ•°å­¦å…¬å¼**:
```python
# é—å¿˜é—¨: å†³å®šä¸¢å¼ƒå¤šå°‘æ—§è®°å¿†
f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)

# è¾“å…¥é—¨: å†³å®šæ·»åŠ å¤šå°‘æ–°ä¿¡æ¯
i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)
CÌƒ_t = tanh(W_C Â· [h_{t-1}, x_t] + b_C)

# æ›´æ–°è®°å¿†ç»†èƒ
C_t = f_t âŠ™ C_{t-1} + i_t âŠ™ CÌƒ_t

# è¾“å‡ºé—¨: å†³å®šè¾“å‡ºå¤šå°‘ä¿¡æ¯
o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)
h_t = o_t âŠ™ tanh(C_t)
```

**ç›´è§‚ç†è§£**:
- **é—å¿˜é—¨**: "å¿˜è®°ä¸é‡è¦çš„ä¿¡æ¯"
- **è¾“å…¥é—¨**: "è®°ä½æ–°çš„é‡è¦ä¿¡æ¯"
- **è¾“å‡ºé—¨**: "è¾“å‡ºå½“å‰éœ€è¦çš„ä¿¡æ¯"

**ä»£ç ç¤ºä¾‹**:
```python
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)

    def forward(self, x):
        # x: (batch, seq_len)
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        lstm_out, (hidden, cell) = self.lstm(embedded)
        # å–æœ€åéšè—çŠ¶æ€
        out = self.fc(hidden[-1])  # (batch, num_classes)
        return out
```

#### 2.4 GRU (Gated Recurrent Unit)

**ç®€åŒ–ç‰ˆLSTM**: åˆå¹¶è®°å¿†ç»†èƒå’Œéšè—çŠ¶æ€ï¼Œåªæœ‰2ä¸ªé—¨ã€‚

**æ•°å­¦å…¬å¼**:
```python
# é‡ç½®é—¨
r_t = Ïƒ(W_r Â· [h_{t-1}, x_t])

# æ›´æ–°é—¨
z_t = Ïƒ(W_z Â· [h_{t-1}, x_t])

# å€™é€‰éšè—çŠ¶æ€
hÌƒ_t = tanh(W Â· [r_t âŠ™ h_{t-1}, x_t])

# æœ€ç»ˆéšè—çŠ¶æ€
h_t = (1 - z_t) âŠ™ h_{t-1} + z_t âŠ™ hÌƒ_t
```

**GRU vs LSTM**:
| ç‰¹æ€§ | LSTM | GRU |
|------|------|-----|
| å‚æ•°é‡ | æ›´å¤šï¼ˆ4ä¸ªé—¨ï¼‰ | æ›´å°‘ï¼ˆ2ä¸ªé—¨ï¼‰ |
| è®­ç»ƒé€Ÿåº¦ | è¾ƒæ…¢ | è¾ƒå¿« |
| æ€§èƒ½ | ç•¥é«˜ï¼ˆå¤§æ•°æ®é›†ï¼‰ | ç•¥ä½ |
| é€‚ç”¨åœºæ™¯ | å¤æ‚ä»»åŠ¡ | ç®€å•ä»»åŠ¡ã€èµ„æºå—é™ |

---

### 3. Transformeræ¶æ„

#### 3.1 ä¸ºä»€ä¹ˆéœ€è¦Transformerï¼Ÿ

**RNN/LSTMçš„å±€é™**:
- âŒ æ— æ³•å¹¶è¡ŒåŒ–ï¼ˆå¿…é¡»æŒ‰æ—¶é—´æ­¥é¡ºåºè®¡ç®—ï¼‰
- âŒ é•¿æœŸä¾èµ–é—®é¢˜ï¼ˆè™½ç„¶LSTMç¼“è§£äº†ï¼Œä½†æœªå½»åº•è§£å†³ï¼‰
- âŒ è®­ç»ƒæ…¢ï¼ˆå°¤å…¶æ˜¯é•¿åºåˆ—ï¼‰

**Transformerçš„ä¼˜åŠ¿**:
- âœ… å®Œå…¨å¹¶è¡ŒåŒ–ï¼ˆæ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—ï¼‰
- âœ… é•¿è·ç¦»ä¾èµ–ç›´æ¥å»ºæ¨¡ï¼ˆSelf-Attentionï¼‰
- âœ… å¯æ‰©å±•æ€§å¼ºï¼ˆé€‚åˆå¤§è§„æ¨¡é¢„è®­ç»ƒï¼‰

#### 3.2 Self-Attentionæœºåˆ¶

**æ ¸å¿ƒæ€æƒ³**: è®¡ç®—åºåˆ—ä¸­æ¯ä¸ªè¯ä¸å…¶ä»–æ‰€æœ‰è¯çš„å…³è”ç¨‹åº¦ã€‚

**è®¡ç®—æ­¥éª¤**:

**1) ç”ŸæˆQ, K, VçŸ©é˜µ**:
```
Query  = X Â· W_Q  # (seq_len, d_model) Ã— (d_model, d_k)
Key    = X Â· W_K
Value  = X Â· W_V
```

**2) è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°**:
```
Attention(Q, K, V) = softmax(Q Â· K^T / âˆšd_k) Â· V
```

**ç›´è§‚ç†è§£**:
```
è¾“å…¥å¥å­: "The cat sat on the mat"

å¯¹äºå•è¯"cat":
- Q_cat ä¸æ‰€æœ‰ K è®¡ç®—ç›¸ä¼¼åº¦
- å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°: [0.1, 0.6, 0.2, 0.05, 0.05]
               (The, cat, sat, on,  the)
- å¯¹ V åŠ æƒæ±‚å’Œå¾—åˆ° cat çš„æ–°è¡¨ç¤º
```

**å¯è§†åŒ–**:
```
     The   cat   sat   on    the   mat
The  0.5   0.2   0.1   0.1   0.05  0.05
cat  0.1   0.6   0.2   0.05  0.05  0.0
sat  0.1   0.3   0.4   0.1   0.05  0.05
...
```

**ä»£ç å®ç°**:
```python
import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.embed_dim = embed_dim
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        # x: (batch, seq_len, embed_dim)
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)

        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.embed_dim, dtype=torch.float32))
        attention_weights = torch.softmax(scores, dim=-1)

        # åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)
        return output, attention_weights
```

#### 3.3 Multi-Head Attention

**æ ¸å¿ƒæ€æƒ³**: å¤šä¸ªæ³¨æ„åŠ›å¤´å¹¶è¡Œå­¦ä¹ ä¸åŒçš„ç‰¹å¾å­ç©ºé—´ã€‚

**å…¬å¼**:
```
MultiHead(Q, K, V) = Concat(head_1, ..., head_h) Â· W_O

å…¶ä¸­ head_i = Attention(QÂ·W_Q^i, KÂ·W_K^i, VÂ·W_V^i)
```

**ä¼˜åŠ¿**:
- ä¸åŒå¤´å…³æ³¨ä¸åŒçš„è¯­ä¹‰ä¿¡æ¯
  - Head 1: è¯­æ³•å…³ç³»ï¼ˆä¸»è°“å®¾ï¼‰
  - Head 2: è¯­ä¹‰å…³ç³»ï¼ˆåŒä¹‰è¯ï¼‰
  - Head 3: ä½ç½®å…³ç³»ï¼ˆç›¸é‚»è¯ï¼‰

**ç¤ºä¾‹**ï¼ˆ8ä¸ªå¤´ï¼‰:
```
Head 1: "cat" å…³æ³¨ "sat" (åŠ¨ä½œå…³ç³»)
Head 2: "cat" å…³æ³¨ "the" (ä¿®é¥°å…³ç³»)
Head 3: "cat" å…³æ³¨ "mat" (ä½ç½®å…³ç³»)
...
```

**ä»£ç å®ç°**:
```python
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads

        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        self.out = nn.Linear(embed_dim, embed_dim)

    def forward(self, x):
        batch_size, seq_len, embed_dim = x.size()

        # åˆ†å‰²æˆå¤šä¸ªå¤´
        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # è®¡ç®—æ³¨æ„åŠ›ï¼ˆå¹¶è¡Œè®¡ç®—æ‰€æœ‰å¤´ï¼‰
        scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))
        attention_weights = torch.softmax(scores, dim=-1)
        output = torch.matmul(attention_weights, V)

        # åˆå¹¶å¤šä¸ªå¤´
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)
        return self.out(output)
```

#### 3.4 ä½ç½®ç¼–ç  (Positional Encoding)

**é—®é¢˜**: Self-Attentionå¯¹è¯åºä¸æ•æ„Ÿï¼ˆ"cat sat" å’Œ "sat cat" ç»“æœç›¸åŒï¼‰

**è§£å†³æ–¹æ¡ˆ**: æ·»åŠ ä½ç½®ä¿¡æ¯

**æ­£å¼¦ä½ç½®ç¼–ç **:
```python
PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))
```

**ä»£ç å®ç°**:
```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe.unsqueeze(0))

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]
```

#### 3.5 å®Œæ•´Transformeræ¶æ„

**Encoder-Decoderç»“æ„**:
```
è¾“å…¥åºåˆ— â†’ Encoder (NÃ—) â†’ Decoder (NÃ—) â†’ è¾“å‡ºåºåˆ—
```

**Encoder Block**:
```
è¾“å…¥
  â†“
Multi-Head Attention â†’ Add & Norm
  â†“
Feed Forward â†’ Add & Norm
  â†“
è¾“å‡º
```

**Decoder Block**:
```
è¾“å…¥
  â†“
Masked Multi-Head Attention â†’ Add & Norm
  â†“
Cross-Attention (with Encoder) â†’ Add & Norm
  â†“
Feed Forward â†’ Add & Norm
  â†“
è¾“å‡º
```

**å…³é”®ç»„ä»¶**:
- **æ®‹å·®è¿æ¥ (Residual Connection)**: ç¼“è§£æ¢¯åº¦æ¶ˆå¤±
- **Layer Normalization**: ç¨³å®šè®­ç»ƒ
- **Feed Forward Network**: 2å±‚å…¨è¿æ¥ + ReLU
- **Masked Attention**: Decoderä¸­é˜²æ­¢çœ‹åˆ°æœªæ¥ä¿¡æ¯

---

### 4. é¢„è®­ç»ƒæ¨¡å‹

#### 4.1 é¢„è®­ç»ƒ + å¾®è°ƒèŒƒå¼

**ä¼ ç»Ÿæ–¹æ³•**ï¼ˆä»é›¶è®­ç»ƒï¼‰:
```
æ ‡æ³¨æ•°æ®(å°‘) â†’ è®­ç»ƒæ¨¡å‹ â†’ æ€§èƒ½ä¸€èˆ¬
```

**é¢„è®­ç»ƒ + å¾®è°ƒ**:
```
å¤§è§„æ¨¡æ— æ ‡æ³¨æ•°æ® â†’ é¢„è®­ç»ƒ â†’ é€šç”¨è¯­è¨€æ¨¡å‹
         â†“
  å°è§„æ¨¡æ ‡æ³¨æ•°æ® â†’ å¾®è°ƒ â†’ ç‰¹å®šä»»åŠ¡æ¨¡å‹ (é«˜æ€§èƒ½)
```

#### 4.2 BERT (Bidirectional Encoder Representations from Transformers)

**æ ¸å¿ƒæ€æƒ³**: åŒå‘ç¼–ç å™¨ï¼Œä½¿ç”¨**Masked Language Model**é¢„è®­ç»ƒã€‚

**é¢„è®­ç»ƒä»»åŠ¡**:

**1) Masked Language Model (MLM)**:
```
è¾“å…¥: "The [MASK] sat on the mat"
ç›®æ ‡: é¢„æµ‹ [MASK] = "cat"
```

**2) Next Sentence Prediction (NSP)**:
```
è¾“å…¥: [CLS] Sentence A [SEP] Sentence B [SEP]
ç›®æ ‡: åˆ¤æ–­Bæ˜¯å¦æ˜¯Açš„ä¸‹ä¸€å¥
```

**æ¨¡å‹æ¶æ„**:
```
è¾“å…¥: [CLS] token1 token2 ... tokenN [SEP]
  â†“
Transformer Encoder (12å±‚ or 24å±‚)
  â†“
è¾“å‡º: [CLS]è¡¨ç¤º + tokenè¡¨ç¤º
```

**BERTå®¶æ—**:
| æ¨¡å‹ | å±‚æ•° | éšè—ç»´åº¦ | æ³¨æ„åŠ›å¤´ | å‚æ•°é‡ |
|------|------|---------|---------|--------|
| BERT-Base | 12 | 768 | 12 | 110M |
| BERT-Large | 24 | 1024 | 16 | 340M |
| RoBERTa | 24 | 1024 | 16 | 355M (ä¼˜åŒ–ç‰ˆBERT) |
| ALBERT | 12 | 768 | 12 | 12M (å‚æ•°å…±äº«) |

**åº”ç”¨åœºæ™¯**:
- æ–‡æœ¬åˆ†ç±»
- å‘½åå®ä½“è¯†åˆ« (NER)
- é—®ç­”ç³»ç»Ÿ
- è¯­ä¹‰ç›¸ä¼¼åº¦

**å¾®è°ƒç¤ºä¾‹** (ä½¿ç”¨Hugging Face):
```python
from transformers import BertForSequenceClassification, BertTokenizer, Trainer

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# å‡†å¤‡æ•°æ®
texts = ["I love NLP", "This is terrible"]
labels = [1, 0]

inputs = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")
inputs['labels'] = torch.tensor(labels)

# å¾®è°ƒ
trainer = Trainer(model=model, train_dataset=inputs)
trainer.train()
```

#### 4.3 GPT (Generative Pre-trained Transformer)

**æ ¸å¿ƒæ€æƒ³**: å•å‘è§£ç å™¨ï¼Œä½¿ç”¨**è‡ªå›å½’è¯­è¨€æ¨¡å‹**é¢„è®­ç»ƒã€‚

**é¢„è®­ç»ƒä»»åŠ¡**: **Causal Language Modeling**
```
è¾“å…¥: "The cat sat"
ç›®æ ‡: é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ "on"
```

**æ¨¡å‹æ¶æ„**:
```
è¾“å…¥: token1 token2 ... tokenN
  â†“
Transformer Decoder (ä½¿ç”¨Masked Self-Attention)
  â†“
è¾“å‡º: é¢„æµ‹ä¸‹ä¸€ä¸ªtokençš„æ¦‚ç‡åˆ†å¸ƒ
```

**GPTæ¼”è¿›**:
| æ¨¡å‹ | å‘å¸ƒå¹´ä»½ | å±‚æ•° | å‚æ•°é‡ | å…³é”®ç‰¹æ€§ |
|------|---------|------|--------|---------|
| GPT-1 | 2018 | 12 | 117M | é¦–æ¬¡æå‡ºé¢„è®­ç»ƒ+å¾®è°ƒ |
| GPT-2 | 2019 | 48 | 1.5B | Zero-shotå­¦ä¹ èƒ½åŠ› |
| GPT-3 | 2020 | 96 | 175B | Few-shot in-context learning |
| GPT-4 | 2023 | ? | >1T | å¤šæ¨¡æ€ã€æ›´å¼ºæ¨ç†èƒ½åŠ› |

**GPT vs BERT**:
| ç‰¹æ€§ | BERT | GPT |
|------|------|-----|
| æ¶æ„ | Encoder-only | Decoder-only |
| æ³¨æ„åŠ›æ–¹å‘ | åŒå‘ | å•å‘ï¼ˆå› æœï¼‰ |
| é¢„è®­ç»ƒä»»åŠ¡ | MLM + NSP | è‡ªå›å½’LM |
| é€‚ç”¨ä»»åŠ¡ | ç†è§£ç±»ï¼ˆåˆ†ç±»ã€NERï¼‰ | ç”Ÿæˆç±»ï¼ˆæ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ï¼‰ |

#### 4.4 T5 (Text-to-Text Transfer Transformer)

**æ ¸å¿ƒæ€æƒ³**: å°†æ‰€æœ‰NLPä»»åŠ¡ç»Ÿä¸€ä¸º**æ–‡æœ¬åˆ°æ–‡æœ¬**è½¬æ¢ã€‚

**ä»»åŠ¡ç»Ÿä¸€æ ¼å¼**:
```python
# ç¿»è¯‘
"translate English to German: Hello" â†’ "Hallo"

# åˆ†ç±»
"sentiment: This movie is great!" â†’ "positive"

# æ‘˜è¦
"summarize: [é•¿æ–‡æœ¬]" â†’ "ç®€çŸ­æ‘˜è¦"

# é—®ç­”
"question: What is NLP? context: ..." â†’ "ç­”æ¡ˆ"
```

**ä¼˜åŠ¿**:
- ç»Ÿä¸€çš„è¾“å…¥è¾“å‡ºæ ¼å¼
- ä¸€ä¸ªæ¨¡å‹å¤„ç†å¤šä¸ªä»»åŠ¡
- å¯ä»¥è½»æ¾æ·»åŠ æ–°ä»»åŠ¡

**T5å®¶æ—**:
- T5-Small: 60Må‚æ•°
- T5-Base: 220Må‚æ•°
- T5-Large: 770Må‚æ•°
- T5-3B: 3Bå‚æ•°
- T5-11B: 11Bå‚æ•°

---

### 5. ä¸‹æ¸¸ä»»åŠ¡ä¸å¾®è°ƒ

#### 5.1 æ–‡æœ¬åˆ†ç±» (Text Classification)

**ä»»åŠ¡**: å°†æ–‡æœ¬åˆ†é…åˆ°é¢„å®šä¹‰ç±»åˆ«

**åº”ç”¨åœºæ™¯**:
- æƒ…æ„Ÿåˆ†æï¼ˆæ­£é¢/è´Ÿé¢ï¼‰
- åƒåœ¾é‚®ä»¶æ£€æµ‹
- æ–°é—»åˆ†ç±»

**å¾®è°ƒç­–ç•¥**:
```python
# ä½¿ç”¨[CLS] tokençš„è¡¨ç¤ºè¿›è¡Œåˆ†ç±»
[CLS] token1 token2 ... [SEP]
  â†“
BERT Encoder
  â†“
[CLS]è¡¨ç¤º â†’ Linear â†’ Softmax â†’ ç±»åˆ«æ¦‚ç‡
```

#### 5.2 å‘½åå®ä½“è¯†åˆ« (NER)

**ä»»åŠ¡**: è¯†åˆ«æ–‡æœ¬ä¸­çš„å®ä½“ï¼ˆäººåã€åœ°åã€ç»„ç»‡åç­‰ï¼‰

**æ ‡æ³¨æ ¼å¼** (BIO):
```
I      B-PER  (Begin-Person)
love   O      (Outside)
New    B-LOC  (Begin-Location)
York   I-LOC  (Inside-Location)
```

**å¾®è°ƒç­–ç•¥**:
```python
# å¯¹æ¯ä¸ªtokenè¿›è¡Œåˆ†ç±»
token1 token2 ... tokenN
  â†“         â†“         â†“
BERT Encoder
  â†“         â†“         â†“
Linear   Linear    Linear
  â†“         â†“         â†“
B-PER     O        B-LOC
```

**ä»£ç ç¤ºä¾‹**:
```python
from transformers import BertForTokenClassification

model = BertForTokenClassification.from_pretrained(
    'bert-base-uncased',
    num_labels=len(label_list)  # B-PER, I-PER, B-LOC, ...
)
```

#### 5.3 æœºå™¨ç¿»è¯‘ (Machine Translation)

**ä»»åŠ¡**: å°†æºè¯­è¨€æ–‡æœ¬ç¿»è¯‘æˆç›®æ ‡è¯­è¨€

**Transformeræ¶æ„**:
```
æºè¯­è¨€è¾“å…¥ â†’ Encoder â†’ Decoder â†’ ç›®æ ‡è¯­è¨€è¾“å‡º
```

**è®­ç»ƒç­–ç•¥**:
- **Teacher Forcing**: è®­ç»ƒæ—¶ä½¿ç”¨çœŸå®ç›®æ ‡ä½œä¸ºDecoderè¾“å…¥
- **Beam Search**: æ¨ç†æ—¶ä¿ç•™top-kå€™é€‰ç¿»è¯‘

**ä»£ç ç¤ºä¾‹**:
```python
from transformers import MarianMTModel, MarianTokenizer

# åŠ è½½é¢„è®­ç»ƒç¿»è¯‘æ¨¡å‹
model_name = "Helsinki-NLP/opus-mt-en-zh"
model = MarianMTModel.from_pretrained(model_name)
tokenizer = MarianTokenizer.from_pretrained(model_name)

# ç¿»è¯‘
text = "Hello, how are you?"
inputs = tokenizer(text, return_tensors="pt")
translated = model.generate(**inputs)
print(tokenizer.decode(translated[0], skip_special_tokens=True))
# è¾“å‡º: "ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ"
```

---

## ğŸ› ï¸ å®è·µç¯èŠ‚

### ä»»åŠ¡1: è¯åµŒå…¥å¯è§†åŒ–

**ç›®æ ‡**: ä½¿ç”¨t-SNEå¯è§†åŒ–Word2Vecè¯å‘é‡ï¼Œè§‚å¯Ÿè¯­ä¹‰èšç±»

**å…³é”®ä»£ç ** (`notebooks/stage4/04-rnn-text-classification.ipynb` ç¬¬2èŠ‚):
```python
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# æå–è¯å‘é‡
words = ["king", "queen", "man", "woman", "apple", "orange"]
vectors = [model.wv[word] for word in words]

# é™ç»´åˆ°2D
tsne = TSNE(n_components=2, random_state=42)
vectors_2d = tsne.fit_transform(vectors)

# å¯è§†åŒ–
plt.figure(figsize=(10, 8))
for i, word in enumerate(words):
    plt.scatter(vectors_2d[i, 0], vectors_2d[i, 1])
    plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]))
plt.show()
```

**é¢„æœŸç»“æœ**: "king"-"queen" è·ç¦»è¿‘ï¼Œ"apple"-"orange" è·ç¦»è¿‘

### ä»»åŠ¡2: ä½¿ç”¨LSTMè¿›è¡Œæƒ…æ„Ÿåˆ†ç±»

**ç›®æ ‡**: åœ¨IMDBç”µå½±è¯„è®ºæ•°æ®é›†ä¸Šè®­ç»ƒLSTMåˆ†ç±»å™¨

**æ­¥éª¤**:
1. åŠ è½½IMDBæ•°æ®é›†
2. æ–‡æœ¬é¢„å¤„ç†ï¼ˆåˆ†è¯ã€æˆªæ–­/å¡«å……ï¼‰
3. æ„å»ºLSTMæ¨¡å‹
4. è®­ç»ƒå¹¶è¯„ä¼°

**é¢„æœŸç»“æœ**: æµ‹è¯•é›†å‡†ç¡®ç‡ > 85%

### ä»»åŠ¡3: å¾®è°ƒBERTè¿›è¡Œæ–‡æœ¬åˆ†ç±»

**ç›®æ ‡**: ä½¿ç”¨Hugging Face Transformerså¾®è°ƒBERT

**æ­¥éª¤**:
1. åŠ è½½é¢„è®­ç»ƒBERTæ¨¡å‹
2. å‡†å¤‡æ•°æ®é›†ï¼ˆtokenizationï¼‰
3. å®šä¹‰è®­ç»ƒå‚æ•°
4. ä½¿ç”¨Trainer APIå¾®è°ƒ
5. è¯„ä¼°æ€§èƒ½

**é¢„æœŸç»“æœ**:
- 5 epochså†…éªŒè¯é›†å‡†ç¡®ç‡ > 90%
- å¯¹æ¯”LSTM: BERTå‡†ç¡®ç‡æå‡5-10%

### ä»»åŠ¡4: å¯è§†åŒ–Attentionæƒé‡

**ç›®æ ‡**: ç†è§£Transformerå¦‚ä½•å…³æ³¨ä¸åŒè¯è¯­

**ä»£ç **:
```python
from transformers import BertTokenizer, BertModel
import matplotlib.pyplot as plt

model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "The cat sat on the mat"
inputs = tokenizer(text, return_tensors="pt")
outputs = model(**inputs)

# æå–ç¬¬ä¸€å±‚ç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡
attention = outputs.attentions[0][0, 0].detach().numpy()

# å¯è§†åŒ–
plt.imshow(attention, cmap='viridis')
plt.xticks(range(len(tokens)), tokens, rotation=45)
plt.yticks(range(len(tokens)), tokens)
plt.colorbar()
plt.show()
```

---

## ğŸ“– æ‰©å±•é˜…è¯»

### ç»å…¸è®ºæ–‡

1. **Attention Is All You Need** (Transformer, 2017)
   - é“¾æ¥: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
   - é˜…è¯»æ—¶é—´: 2å°æ—¶

2. **BERT: Pre-training of Deep Bidirectional Transformers** (2018)
   - é“¾æ¥: [https://arxiv.org/abs/1810.04805](https://arxiv.org/abs/1810.04805)
   - é˜…è¯»æ—¶é—´: 1.5å°æ—¶

3. **Language Models are Few-Shot Learners** (GPT-3, 2020)
   - é“¾æ¥: [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)
   - é˜…è¯»æ—¶é—´: 2å°æ—¶

### åœ¨çº¿èµ„æº

- **CS224N (Stanford)**: [http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)
- **Hugging Face Course**: [https://huggingface.co/course/](https://huggingface.co/course/)
- **The Illustrated Transformer**: [https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)

### å®æˆ˜é¡¹ç›®æ¨è

å®Œæˆæœ¬æ¨¡å—åï¼Œå»ºè®®å°è¯•ä»¥ä¸‹é¡¹ç›®ï¼š

- ğŸš€ **[P06: Transformerç¿»è¯‘ç³»ç»Ÿ](../projects/p06-transformer-translation/README.md)** - åŒæ¡†æ¶å®ç°ï¼ˆæ¨èï¼‰
- ğŸš€ **[P07: é¢„è®­ç»ƒæ¨¡å‹ä¿¡æ¯æå–](../projects/p07-pretrained-info-extraction/README.md)** - BERTå¾®è°ƒ

---

## â“ å¸¸è§é—®é¢˜ (FAQ)

### Q1: LSTMå’ŒTransformerå¦‚ä½•é€‰æ‹©ï¼Ÿ

**A**: æ ¹æ®ä»»åŠ¡å’Œæ•°æ®è§„æ¨¡é€‰æ‹©ï¼š
- **å°æ•°æ®é›† (<10kæ ·æœ¬)**: LSTM/GRU (å‚æ•°å°‘ï¼Œä¸æ˜“è¿‡æ‹Ÿåˆ)
- **å¤§æ•°æ®é›† (>100kæ ·æœ¬)**: Transformer (æ€§èƒ½æ›´å¥½)
- **å®æ—¶æ¨ç†**: LSTM (æ¨ç†é€Ÿåº¦å¿«)
- **æ‰¹é‡æ¨ç†**: Transformer (å¯å¹¶è¡Œ)

### Q2: å¦‚ä½•å¤„ç†æœªç™»å½•è¯(OOV)ï¼Ÿ

**A**: ä¸‰ç§ç­–ç•¥ï¼š
1. **FastText**: ä½¿ç”¨å­è¯ä¿¡æ¯
2. **WordPiece/BPE**: åˆ†è¯ç®—æ³•ï¼ˆBERTä½¿ç”¨ï¼‰
3. **`<UNK>` token**: æ›¿æ¢ä¸ºç‰¹æ®Šæ ‡è®°

### Q3: ä¸ºä»€ä¹ˆBERTä¸èƒ½ç”Ÿæˆæ–‡æœ¬ï¼Ÿ

**A**: BERTæ˜¯**åŒå‘ç¼–ç å™¨**ï¼Œè®­ç»ƒæ—¶å¯ä»¥çœ‹åˆ°æœªæ¥è¯ï¼Œæ— æ³•ç”¨äºè‡ªå›å½’ç”Ÿæˆã€‚ç”Ÿæˆä»»åŠ¡éœ€è¦ä½¿ç”¨**å•å‘è§£ç å™¨**ï¼ˆå¦‚GPTï¼‰ã€‚

### Q4: å¾®è°ƒBERTæ—¶å¦‚ä½•é¿å…è¿‡æ‹Ÿåˆï¼Ÿ

**A**: 5ä¸ªæŠ€å·§ï¼š
1. ä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ (2e-5 - 5e-5)
2. æ·»åŠ Dropout (0.1 - 0.3)
3. å†»ç»“éƒ¨åˆ†å±‚ï¼ˆåªå¾®è°ƒåå‡ å±‚ï¼‰
4. ä½¿ç”¨Early Stopping
5. æ•°æ®å¢å¼ºï¼ˆå›è¯‘ã€åŒä¹‰è¯æ›¿æ¢ï¼‰

### Q5: Transformerè®­ç»ƒå¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**A**: ä¼˜åŒ–ç­–ç•¥ï¼š
1. **å‡å°‘åºåˆ—é•¿åº¦**: 512 â†’ 128ï¼ˆå¦‚æœä»»åŠ¡å…è®¸ï¼‰
2. **ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯**: æ¨¡æ‹Ÿå¤§batch size
3. **æ··åˆç²¾åº¦è®­ç»ƒ**: FP16ä»£æ›¿FP32
4. **ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹**: é¿å…ä»é›¶è®­ç»ƒ
5. **æ¨¡å‹è’¸é¦**: ç”¨å°æ¨¡å‹å­¦ä¹ å¤§æ¨¡å‹

---

## âœ… å­¦ä¹ æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ åº”è¯¥èƒ½å¤Ÿï¼š

- [ ] è§£é‡ŠWord2Vecçš„Skip-gramå’ŒCBOWè®­ç»ƒæ–¹å¼
- [ ] è¯´æ˜LSTMå¦‚ä½•è§£å†³RNNçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- [ ] æ‰‹åŠ¨è®¡ç®—Self-Attentionçš„è¾“å‡ºï¼ˆç»™å®šQ, K, Vï¼‰
- [ ] è§£é‡ŠMulti-Head Attentionçš„ä¼˜åŠ¿
- [ ] åŒºåˆ†BERTå’ŒGPTçš„æ¶æ„ä¸é¢„è®­ç»ƒä»»åŠ¡
- [ ] ä½¿ç”¨Hugging Face Transformerså¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
- [ ] å¯è§†åŒ–å¹¶è§£é‡ŠAttentionæƒé‡
- [ ] æ¯”è¾ƒä¸åŒNLPæ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„æ€§èƒ½

---

## â­ï¸ ä¸‹ä¸€æ­¥

å®Œæˆæœ¬æ¨¡å—åï¼Œä½ å¯ä»¥ï¼š

1. **å›é¡¾æ€»ç»“**: å¤ä¹ [æ¨¡å—M01](../01-dl-basics/README.md)å’Œ[æ¨¡å—M02](../02-cv-basics/README.md)
2. **å®æˆ˜é¡¹ç›®**: ä»[é¡¹ç›®åˆ—è¡¨](../projects/)ä¸­é€‰æ‹©NLPé¡¹ç›®å¼€å§‹å®è·µ
3. **æ·±å…¥ç ”ç©¶**: é˜…è¯»Transformer/BERT/GPTåŸè®ºæ–‡
4. **è¿›é˜¶å­¦ä¹ **: è¿›å…¥[é˜¶æ®µ5: AIGCä¸å¤§æ¨¡å‹](../../stage5/index.md)

---

**å‡†å¤‡å¥½äº†å—ï¼Ÿæ‰“å¼€ [04-rnn-text-classification.ipynb](../../notebooks/stage4/04-rnn-text-classification.ipynb) å¼€å§‹åŠ¨æ‰‹å®è·µï¼** ğŸš€
