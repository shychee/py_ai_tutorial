#!/usr/bin/env python3
"""
æ•°æ®å®Œæ•´æ€§éªŒè¯è„šæœ¬ (Data Integrity Verification Script)

éªŒè¯å·²ä¸‹è½½æ•°æ®é›†çš„å®Œæ•´æ€§ï¼ˆæ–‡ä»¶å­˜åœ¨æ€§ã€æ ¡éªŒå’Œã€å¤§å°ï¼‰ã€‚

Usage:
    python scripts/data/verify.py --stage 3
    python scripts/data/verify.py --stage all
    python scripts/data/verify.py --checksums-only
"""

import argparse
import hashlib
import sys
from pathlib import Path
from typing import Dict, List, Optional
import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


class DataVerifier:
    """æ•°æ®éªŒè¯å™¨"""

    def __init__(self, data_dir: Path, config_path: Path):
        self.data_dir = data_dir
        self.config_path = config_path
        self.datasets_config: List[Dict] = []

    def load_config(self, stage_filter: Optional[str] = None):
        """åŠ è½½æ•°æ®é›†é…ç½®"""
        with open(self.config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
            if stage_filter and stage_filter != "all":
                self.datasets_config = [
                    ds for ds in config.get("datasets", [])
                    if ds["stage_id"] == f"stage{stage_filter}"
                ]
            else:
                self.datasets_config = config.get("datasets", [])

    def calculate_checksum(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶SHA256æ ¡éªŒå’Œ"""
        sha256_hash = hashlib.sha256()
        try:
            with open(file_path, "rb") as f:
                for byte_block in iter(lambda: f.read(8192), b""):
                    sha256_hash.update(byte_block)
            return sha256_hash.hexdigest()
        except Exception as e:
            print(f"   âŒ æ— æ³•è®¡ç®—æ ¡éªŒå’Œ: {e}")
            return ""

    def verify_file(
        self,
        file_path: Path,
        expected_checksum: str,
        expected_size_mb: Optional[int] = None,
        checksums_only: bool = False
    ) -> Dict[str, any]:
        """éªŒè¯æ–‡ä»¶å®Œæ•´æ€§"""
        result = {
            "exists": file_path.exists(),
            "checksum_valid": False,
            "size_valid": False,
            "actual_size_mb": 0,
            "actual_checksum": "",
        }

        if not result["exists"]:
            return result

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        actual_size_bytes = file_path.stat().st_size
        result["actual_size_mb"] = actual_size_bytes / (1024 ** 2)

        if expected_size_mb:
            # å…è®¸Â±10%è¯¯å·®
            size_tolerance = expected_size_mb * 0.1
            result["size_valid"] = abs(result["actual_size_mb"] - expected_size_mb) <= size_tolerance

        # æ£€æŸ¥æ ¡éªŒå’Œ
        if not checksums_only and expected_checksum != "PLACEHOLDER_CHECKSUM_TO_BE_GENERATED":
            result["actual_checksum"] = self.calculate_checksum(file_path)
            result["checksum_valid"] = (result["actual_checksum"] == expected_checksum)
        elif expected_checksum == "PLACEHOLDER_CHECKSUM_TO_BE_GENERATED":
            result["checksum_valid"] = True  # è·³è¿‡å ä½ç¬¦

        return result

    def verify_dataset(self, dataset_config: Dict, checksums_only: bool = False) -> Dict[str, any]:
        """éªŒè¯å•ä¸ªæ•°æ®é›†"""
        dataset_id = dataset_config["id"]
        stage_id = dataset_config["stage_id"]
        stage_num = stage_id.replace("stage", "")

        result = {
            "dataset_id": dataset_id,
            "dataset_name": dataset_config["name"],
            "files_total": len(dataset_config["files"]),
            "files_verified": 0,
            "files_missing": 0,
            "files_invalid": 0,
            "file_details": [],
        }

        for file_info in dataset_config["files"]:
            filename = file_info["filename"]
            file_path = self.data_dir / stage_id / filename
            expected_checksum = file_info["checksum_sha256"]
            expected_size_mb = file_info.get("size_mb")

            file_result = self.verify_file(
                file_path,
                expected_checksum,
                expected_size_mb,
                checksums_only
            )

            if not file_result["exists"]:
                result["files_missing"] += 1
                status = "âŒ ç¼ºå¤±"
            elif not file_result["checksum_valid"] or not file_result["size_valid"]:
                result["files_invalid"] += 1
                status = "âš ï¸  æ— æ•ˆ"
            else:
                result["files_verified"] += 1
                status = "âœ… å®Œæ•´"

            result["file_details"].append({
                "filename": filename,
                "status": status,
                "exists": file_result["exists"],
                "checksum_valid": file_result["checksum_valid"],
                "size_valid": file_result["size_valid"],
                "actual_size_mb": file_result["actual_size_mb"],
            })

        return result

    def verify_all(self, checksums_only: bool = False, verbose: bool = True) -> Dict[str, any]:
        """éªŒè¯æ‰€æœ‰æ•°æ®é›†"""
        summary = {
            "total_datasets": len(self.datasets_config),
            "verified_datasets": 0,
            "missing_datasets": 0,
            "invalid_datasets": 0,
            "total_files": 0,
            "verified_files": 0,
            "missing_files": 0,
            "invalid_files": 0,
            "dataset_results": [],
        }

        if verbose:
            print("=" * 70)
            print("ğŸ” æ•°æ®å®Œæ•´æ€§éªŒè¯ (Data Integrity Verification)")
            print("=" * 70)
            print(f"æ•°æ®ç›®å½•: {self.data_dir}")
            print(f"æ•°æ®é›†æ•°é‡: {summary['total_datasets']}")
            print()

        for dataset_config in self.datasets_config:
            result = self.verify_dataset(dataset_config, checksums_only)
            summary["dataset_results"].append(result)

            summary["total_files"] += result["files_total"]
            summary["verified_files"] += result["files_verified"]
            summary["missing_files"] += result["files_missing"]
            summary["invalid_files"] += result["files_invalid"]

            if result["files_missing"] == 0 and result["files_invalid"] == 0:
                summary["verified_datasets"] += 1
            elif result["files_missing"] == result["files_total"]:
                summary["missing_datasets"] += 1
            else:
                summary["invalid_datasets"] += 1

            if verbose:
                # æ‰“å°æ•°æ®é›†éªŒè¯ç»“æœ
                status_icon = "âœ…" if result["files_missing"] == 0 and result["files_invalid"] == 0 else "âŒ"
                print(f"{status_icon} {result['dataset_name']} ({result['dataset_id']})")

                for file_detail in result["file_details"]:
                    print(f"   {file_detail['status']} {file_detail['filename']}")
                    if file_detail["exists"]:
                        print(f"      å¤§å°: {file_detail['actual_size_mb']:.2f}MB")
                        if not file_detail["checksum_valid"] and not checksums_only:
                            print(f"      âš ï¸  æ ¡éªŒå’Œä¸åŒ¹é…")
                        if not file_detail["size_valid"]:
                            print(f"      âš ï¸  æ–‡ä»¶å¤§å°å¼‚å¸¸")

                print()

        if verbose:
            print("=" * 70)
            print("ğŸ“Š éªŒè¯æ€»ç»“ (Summary)")
            print("=" * 70)
            print(f"æ•°æ®é›†: {summary['verified_datasets']}/{summary['total_datasets']} å®Œæ•´")
            if summary['missing_datasets'] > 0:
                print(f"        {summary['missing_datasets']} ä¸ªæ•°æ®é›†å®Œå…¨ç¼ºå¤±")
            if summary['invalid_datasets'] > 0:
                print(f"        {summary['invalid_datasets']} ä¸ªæ•°æ®é›†éƒ¨åˆ†ç¼ºå¤±æˆ–æŸå")
            print()
            print(f"æ–‡ä»¶:   {summary['verified_files']}/{summary['total_files']} å®Œæ•´")
            if summary['missing_files'] > 0:
                print(f"        {summary['missing_files']} ä¸ªæ–‡ä»¶ç¼ºå¤±")
            if summary['invalid_files'] > 0:
                print(f"        {summary['invalid_files']} ä¸ªæ–‡ä»¶æŸå")
            print("=" * 70)

            if summary["missing_files"] > 0:
                print()
                print("ğŸ’¡ æç¤º: è¿è¡Œä»¥ä¸‹å‘½ä»¤ä¸‹è½½ç¼ºå¤±çš„æ•°æ®é›†:")
                stages_with_missing = set()
                for ds_result in summary["dataset_results"]:
                    if ds_result["files_missing"] > 0:
                        dataset_id = ds_result["dataset_id"]
                        stage_num = dataset_id.split("-")[1].replace("S", "")
                        stages_with_missing.add(stage_num)

                for stage_num in sorted(stages_with_missing):
                    print(f"   python scripts/data/download-stage{stage_num}.py")

        return summary

    def print_statistics(self):
        """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print("=" * 70)
        print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ (Dataset Statistics)")
        print("=" * 70)
        print()

        by_stage: Dict[str, List] = {}
        for ds in self.datasets_config:
            stage_id = ds["stage_id"]
            if stage_id not in by_stage:
                by_stage[stage_id] = []
            by_stage[stage_id].append(ds)

        for stage_id in sorted(by_stage.keys()):
            datasets = by_stage[stage_id]
            stage_num = stage_id.replace("stage", "")

            total_size_mb = sum(
                file_info["size_mb"]
                for ds in datasets
                for file_info in ds["files"]
            )
            total_files = sum(len(ds["files"]) for ds in datasets)

            print(f"é˜¶æ®µ{stage_num}:")
            print(f"   æ•°æ®é›†æ•°é‡: {len(datasets)}")
            print(f"   æ–‡ä»¶æ•°é‡:   {total_files}")
            print(f"   æ€»å¤§å°:     {total_size_mb / 1024:.2f}GB ({total_size_mb:.0f}MB)")
            print()

        print("=" * 70)


def main():
    parser = argparse.ArgumentParser(
        description="éªŒè¯æ•°æ®é›†å®Œæ•´æ€§"
    )
    parser.add_argument(
        "--stage",
        type=str,
        choices=["3", "4", "5", "all"],
        help="æŒ‡å®šéªŒè¯çš„å­¦ä¹ é˜¶æ®µï¼ˆ3/4/5/allï¼‰",
    )
    parser.add_argument(
        "--checksums-only",
        action="store_true",
        help="ä»…æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§å’Œå¤§å°ï¼Œè·³è¿‡æ ¡éªŒå’Œè®¡ç®—ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰",
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯",
    )
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=PROJECT_ROOT / "data",
        help="æ•°æ®å­˜å‚¨ç›®å½•ï¼ˆé»˜è®¤: ./dataï¼‰",
    )
    parser.add_argument(
        "--config",
        type=Path,
        default=PROJECT_ROOT / "configs" / "content" / "datasets.yaml",
        help="æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    args = parser.parse_args()

    # åˆå§‹åŒ–éªŒè¯å™¨
    verifier = DataVerifier(
        data_dir=args.data_dir,
        config_path=args.config,
    )

    # åŠ è½½é…ç½®
    if not args.config.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        sys.exit(1)

    verifier.load_config(stage_filter=args.stage)

    # ç»Ÿè®¡æ¨¡å¼
    if args.stats:
        verifier.print_statistics()
        sys.exit(0)

    # éªŒè¯æ¨¡å¼
    summary = verifier.verify_all(
        checksums_only=args.checksums_only,
        verbose=True
    )

    # è¿”å›çŠ¶æ€ç 
    if summary["missing_files"] > 0 or summary["invalid_files"] > 0:
        sys.exit(1)
    else:
        sys.exit(0)


if __name__ == "__main__":
    main()
