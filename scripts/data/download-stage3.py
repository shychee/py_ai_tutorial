#!/usr/bin/env python3
"""
é˜¶æ®µ3æ•°æ®ä¸‹è½½è„šæœ¬ (Stage 3 Data Download Script)

ä¸‹è½½é˜¶æ®µ3ï¼ˆæœºå™¨å­¦ä¹ ä¸æ•°æ®æŒ–æ˜ï¼‰æ‰€éœ€çš„9ä¸ªæ•°æ®é›†ã€‚

Usage:
    python scripts/data/download-stage3.py
    python scripts/data/download-stage3.py --dataset DS-S3-P01-HOSPITAL
    python scripts/data/download-stage3.py --verify-only
"""

import argparse
import hashlib
import sys
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urlparse
import yaml

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


class DatasetDownloader:
    """æ•°æ®é›†ä¸‹è½½å™¨"""

    def __init__(self, data_dir: Path, config_path: Path):
        self.data_dir = data_dir
        self.config_path = config_path
        self.datasets_config: List[Dict] = []

    def load_config(self):
        """åŠ è½½æ•°æ®é›†é…ç½®"""
        with open(self.config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
            self.datasets_config = [
                ds for ds in config.get("datasets", [])
                if ds["stage_id"] == "stage3"
            ]

    def calculate_checksum(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶SHA256æ ¡éªŒå’Œ"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()

    def verify_file(self, file_path: Path, expected_checksum: str) -> bool:
        """éªŒè¯æ–‡ä»¶å®Œæ•´æ€§"""
        if not file_path.exists():
            return False

        if expected_checksum == "PLACEHOLDER_CHECKSUM_TO_BE_GENERATED":
            print(f"   âš ï¸  æ ¡éªŒå’Œæœªè®¾ç½®ï¼Œè·³è¿‡éªŒè¯: {file_path.name}")
            return True

        actual_checksum = self.calculate_checksum(file_path)
        return actual_checksum == expected_checksum

    def download_file(self, url: str, dest_path: Path) -> bool:
        """ä¸‹è½½æ–‡ä»¶ï¼ˆä½¿ç”¨urllibï¼‰"""
        try:
            import urllib.request
            print(f"   ğŸ“¥ ä¸‹è½½ä¸­: {url}")
            print(f"   â¬‡ï¸  ä¿å­˜åˆ°: {dest_path}")

            # åˆ›å»ºç›®æ ‡ç›®å½•
            dest_path.parent.mkdir(parents=True, exist_ok=True)

            # ä¸‹è½½æ–‡ä»¶
            urllib.request.urlretrieve(url, dest_path)
            print(f"   âœ… ä¸‹è½½å®Œæˆ: {dest_path.name}")
            return True

        except Exception as e:
            print(f"   âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False

    def download_dataset(self, dataset_id: str) -> bool:
        """ä¸‹è½½å•ä¸ªæ•°æ®é›†"""
        # æŸ¥æ‰¾æ•°æ®é›†é…ç½®
        dataset_config = None
        for ds in self.datasets_config:
            if ds["id"] == dataset_id:
                dataset_config = ds
                break

        if not dataset_config:
            print(f"âŒ æœªæ‰¾åˆ°æ•°æ®é›†: {dataset_id}")
            return False

        print(f"\nğŸ“¦ æ•°æ®é›†: {dataset_config['name']} ({dataset_id})")
        print(f"   é¡¹ç›®: {dataset_config['project_id']}")
        print(f"   æè¿°: {dataset_config['description'][:60]}...")

        # ä¸‹è½½æ–‡ä»¶
        for file_info in dataset_config["files"]:
            filename = file_info["filename"]
            file_path = self.data_dir / "stage3" / filename
            expected_checksum = file_info["checksum_sha256"]

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ä¸”å®Œæ•´
            if file_path.exists():
                print(f"   ğŸ“„ æ–‡ä»¶å·²å­˜åœ¨: {filename}")
                if self.verify_file(file_path, expected_checksum):
                    print(f"   âœ… æ ¡éªŒé€šè¿‡ï¼Œè·³è¿‡ä¸‹è½½")
                    continue
                else:
                    print(f"   âš ï¸  æ ¡éªŒå¤±è´¥ï¼Œé‡æ–°ä¸‹è½½")

            # ä¸‹è½½æ–‡ä»¶
            download_url = dataset_config["source"]["url"]

            # æ³¨æ„ï¼šå®é™…å®ç°æ—¶ï¼Œè¿™é‡Œéœ€è¦çœŸå®çš„ä¸‹è½½URL
            # ç›®å‰ä½¿ç”¨placeholderæ ‡è®°éœ€è¦æ‰‹åŠ¨å¤„ç†
            if "github.com" in download_url or "releases/download" in download_url:
                success = self.download_file(download_url, file_path)
                if not success:
                    # å°è¯•é•œåƒURL
                    mirror_url = dataset_config["source"].get("mirror_url")
                    if mirror_url:
                        print(f"   ğŸ”„ å°è¯•é•œåƒåœ°å€...")
                        success = self.download_file(mirror_url, file_path)

                if success and expected_checksum != "PLACEHOLDER_CHECKSUM_TO_BE_GENERATED":
                    # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
                    if self.verify_file(file_path, expected_checksum):
                        print(f"   âœ… æ–‡ä»¶éªŒè¯é€šè¿‡")
                    else:
                        print(f"   âŒ æ–‡ä»¶éªŒè¯å¤±è´¥ï¼Œæ ¡éªŒå’Œä¸åŒ¹é…")
                        return False
            else:
                print(f"   âš ï¸  æ•°æ®é›†å°šæœªå‘å¸ƒï¼Œè¯·è®¿é—®: {download_url}")
                print(f"   ğŸ’¡ æç¤º: æ•°æ®é›†å°†åœ¨æ•™ç¨‹æ­£å¼å‘å¸ƒæ—¶æä¾›ä¸‹è½½é“¾æ¥")
                # åˆ›å»ºå ä½ç¬¦æ–‡ä»¶ï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰
                self._create_placeholder_file(file_path, file_info)
                return True

        return True

    def _create_placeholder_file(self, file_path: Path, file_info: Dict):
        """åˆ›å»ºå ä½ç¬¦CSVæ–‡ä»¶ï¼ˆç”¨äºå¼€å‘æµ‹è¯•ï¼‰"""
        file_path.parent.mkdir(parents=True, exist_ok=True)

        # ç®€å•CSVå ä½ç¬¦
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("# å ä½ç¬¦æ•°æ®æ–‡ä»¶\n")
            f.write(f"# æ–‡ä»¶å: {file_info['filename']}\n")
            f.write(f"# å¤§å°: {file_info['size_mb']}MB\n")
            f.write(f"# è¡Œæ•°: {file_info['rows']}\n")
            f.write(f"# åˆ—æ•°: {file_info['columns']}\n")
            f.write("# æ­¤æ–‡ä»¶ä¸ºå ä½ç¬¦ï¼Œå®é™…æ•°æ®å°†åœ¨æ•™ç¨‹å‘å¸ƒæ—¶æä¾›\n")

        print(f"   ğŸ“ åˆ›å»ºå ä½ç¬¦æ–‡ä»¶: {file_path}")

    def download_all(self) -> bool:
        """ä¸‹è½½æ‰€æœ‰é˜¶æ®µ3æ•°æ®é›†"""
        print("=" * 60)
        print("ğŸ“š é˜¶æ®µ3æ•°æ®ä¸‹è½½ (Stage 3 Data Download)")
        print("=" * 60)
        print(f"æ•°æ®ç›®å½•: {self.data_dir}/stage3")
        print(f"æ•°æ®é›†æ•°é‡: {len(self.datasets_config)}")
        print()

        success_count = 0
        for dataset in self.datasets_config:
            if self.download_dataset(dataset["id"]):
                success_count += 1

        print("\n" + "=" * 60)
        print(f"âœ… ä¸‹è½½å®Œæˆ: {success_count}/{len(self.datasets_config)} ä¸ªæ•°æ®é›†")
        print("=" * 60)

        return success_count == len(self.datasets_config)

    def verify_all(self) -> bool:
        """éªŒè¯æ‰€æœ‰å·²ä¸‹è½½çš„æ•°æ®é›†"""
        print("=" * 60)
        print("ğŸ” æ•°æ®éªŒè¯ (Data Verification)")
        print("=" * 60)
        print()

        verified_count = 0
        missing_count = 0

        for dataset in self.datasets_config:
            dataset_id = dataset["id"]
            print(f"ğŸ“¦ {dataset['name']} ({dataset_id})")

            for file_info in dataset["files"]:
                filename = file_info["filename"]
                file_path = self.data_dir / "stage3" / filename
                expected_checksum = file_info["checksum_sha256"]

                if not file_path.exists():
                    print(f"   âŒ æ–‡ä»¶ç¼ºå¤±: {filename}")
                    missing_count += 1
                elif self.verify_file(file_path, expected_checksum):
                    print(f"   âœ… æ–‡ä»¶å®Œæ•´: {filename}")
                    verified_count += 1
                else:
                    print(f"   âŒ æ ¡éªŒå¤±è´¥: {filename}")

        print("\n" + "=" * 60)
        print(f"éªŒè¯ç»“æœ: {verified_count} ä¸ªæ–‡ä»¶å®Œæ•´, {missing_count} ä¸ªæ–‡ä»¶ç¼ºå¤±")
        print("=" * 60)

        return missing_count == 0


def main():
    parser = argparse.ArgumentParser(
        description="ä¸‹è½½é˜¶æ®µ3ï¼ˆæœºå™¨å­¦ä¹ ä¸æ•°æ®æŒ–æ˜ï¼‰æ•°æ®é›†"
    )
    parser.add_argument(
        "--dataset",
        type=str,
        help="æŒ‡å®šä¸‹è½½å•ä¸ªæ•°æ®é›†ï¼ˆä¾‹å¦‚: DS-S3-P01-HOSPITALï¼‰",
    )
    parser.add_argument(
        "--verify-only",
        action="store_true",
        help="ä»…éªŒè¯å·²ä¸‹è½½çš„æ•°æ®é›†ï¼Œä¸ä¸‹è½½",
    )
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=PROJECT_ROOT / "data",
        help="æ•°æ®å­˜å‚¨ç›®å½•ï¼ˆé»˜è®¤: ./dataï¼‰",
    )
    parser.add_argument(
        "--config",
        type=Path,
        default=PROJECT_ROOT / "configs" / "content" / "datasets.yaml",
        help="æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„",
    )
    args = parser.parse_args()

    # åˆå§‹åŒ–ä¸‹è½½å™¨
    downloader = DatasetDownloader(
        data_dir=args.data_dir,
        config_path=args.config,
    )

    # åŠ è½½é…ç½®
    if not args.config.exists():
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        sys.exit(1)

    downloader.load_config()

    # éªŒè¯æ¨¡å¼
    if args.verify_only:
        success = downloader.verify_all()
        sys.exit(0 if success else 1)

    # ä¸‹è½½æ¨¡å¼
    if args.dataset:
        # ä¸‹è½½å•ä¸ªæ•°æ®é›†
        success = downloader.download_dataset(args.dataset)
        sys.exit(0 if success else 1)
    else:
        # ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
        success = downloader.download_all()
        sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
