#!/usr/bin/env python3
"""
å¹³å°æ£€æµ‹è„šæœ¬ (Platform Detection Script)

æ£€æµ‹å½“å‰è¿è¡Œç¯å¢ƒçš„æ“ä½œç³»ç»Ÿã€ç¡¬ä»¶ã€Pythonç‰ˆæœ¬ã€GPUæ”¯æŒç­‰ä¿¡æ¯ï¼Œ
ä¸ºç”¨æˆ·æ¨èæœ€ä½³çš„å­¦ä¹ é˜¶æ®µå’Œå®‰è£…ä¾èµ–ã€‚

Usage:
    python scripts/env/detect-platform.py
    python scripts/env/detect-platform.py --json
    python scripts/env/detect-platform.py --recommend
"""

import argparse
import json
import platform
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any


class PlatformDetector:
    """å¹³å°æ£€æµ‹å™¨"""

    def __init__(self):
        self.info: Dict[str, Any] = {}

    def detect_os(self) -> Dict[str, str]:
        """æ£€æµ‹æ“ä½œç³»ç»Ÿä¿¡æ¯"""
        os_info = {
            "system": platform.system(),  # Darwin, Linux, Windows
            "release": platform.release(),
            "version": platform.version(),
            "machine": platform.machine(),  # x86_64, arm64, AMD64
            "processor": platform.processor(),
        }

        # åˆ¤æ–­å…·ä½“å¹³å°
        if os_info["system"] == "Darwin":
            # macOS
            if os_info["machine"] == "arm64":
                os_info["platform"] = "macOS-ARM64"
                os_info["platform_name"] = "macOS Apple Silicon (M1/M2/M3)"
            else:
                os_info["platform"] = "macOS-Intel"
                os_info["platform_name"] = "macOS Intel (x86_64)"
        elif os_info["system"] == "Linux":
            # Linux
            os_info["platform"] = "Linux"
            os_info["platform_name"] = "Linux (Ubuntu/CentOS/etc.)"
            # æ£€æµ‹WSL2
            try:
                with open("/proc/version", "r") as f:
                    version_info = f.read().lower()
                    if "microsoft" in version_info or "wsl" in version_info:
                        os_info["platform"] = "WSL2"
                        os_info["platform_name"] = "Windows WSL2"
            except FileNotFoundError:
                pass
        elif os_info["system"] == "Windows":
            # Windows Native
            os_info["platform"] = "Windows"
            os_info["platform_name"] = "Windows 10/11 Native"
        else:
            os_info["platform"] = "Unknown"
            os_info["platform_name"] = "Unknown Platform"

        return os_info

    def detect_python(self) -> Dict[str, str]:
        """æ£€æµ‹Pythonç‰ˆæœ¬ä¿¡æ¯"""
        python_info = {
            "version": platform.python_version(),
            "version_tuple": list(sys.version_info[:3]),
            "implementation": platform.python_implementation(),
            "compiler": platform.python_compiler(),
        }

        # æ£€æŸ¥Pythonç‰ˆæœ¬æ˜¯å¦ç¬¦åˆè¦æ±‚
        major, minor, _ = sys.version_info[:3]
        python_info["meets_requirement"] = (major == 3 and minor >= 9)
        python_info["recommended"] = (major == 3 and minor >= 11)

        return python_info

    def detect_gpu(self) -> Dict[str, Any]:
        """æ£€æµ‹GPUä¿¡æ¯"""
        gpu_info = {
            "nvidia_available": False,
            "cuda_version": None,
            "mps_available": False,  # Apple Metal Performance Shaders
            "gpu_count": 0,
            "gpu_names": [],
        }

        # æ£€æµ‹NVIDIA GPU (CUDA)
        try:
            result = subprocess.run(
                ["nvidia-smi", "--query-gpu=name,memory.total", "--format=csv,noheader"],
                capture_output=True,
                text=True,
                timeout=5,
            )
            if result.returncode == 0:
                gpu_info["nvidia_available"] = True
                lines = result.stdout.strip().split("\n")
                gpu_info["gpu_count"] = len(lines)
                gpu_info["gpu_names"] = [line.split(",")[0].strip() for line in lines]

                # è·å–CUDAç‰ˆæœ¬
                cuda_result = subprocess.run(
                    ["nvidia-smi"], capture_output=True, text=True, timeout=5
                )
                if "CUDA Version" in cuda_result.stdout:
                    import re
                    match = re.search(r"CUDA Version:\s+([\d.]+)", cuda_result.stdout)
                    if match:
                        gpu_info["cuda_version"] = match.group(1)
        except (FileNotFoundError, subprocess.TimeoutExpired):
            pass

        # æ£€æµ‹Apple MPS (Metal Performance Shaders)
        if platform.system() == "Darwin" and platform.machine() == "arm64":
            try:
                import torch
                if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
                    gpu_info["mps_available"] = True
                    gpu_info["gpu_count"] = 1
                    gpu_info["gpu_names"] = ["Apple GPU (MPS)"]
            except ImportError:
                # PyTorchæœªå®‰è£…ï¼Œå‡è®¾MPSå¯ç”¨
                gpu_info["mps_available"] = True
                gpu_info["gpu_count"] = 1
                gpu_info["gpu_names"] = ["Apple GPU (MPS, PyTorch not installed)"]

        return gpu_info

    def detect_memory(self) -> Dict[str, Any]:
        """æ£€æµ‹å†…å­˜ä¿¡æ¯"""
        memory_info = {
            "total_gb": 0,
            "available_gb": 0,
        }

        try:
            if platform.system() == "Linux" or platform.system() == "Darwin":
                # Linux/macOS
                if platform.system() == "Linux":
                    with open("/proc/meminfo", "r") as f:
                        lines = f.readlines()
                        for line in lines:
                            if line.startswith("MemTotal:"):
                                memory_info["total_gb"] = int(line.split()[1]) / (1024 ** 2)
                            elif line.startswith("MemAvailable:"):
                                memory_info["available_gb"] = int(line.split()[1]) / (1024 ** 2)
                else:
                    # macOS
                    result = subprocess.run(
                        ["sysctl", "hw.memsize"],
                        capture_output=True,
                        text=True,
                        timeout=5,
                    )
                    if result.returncode == 0:
                        mem_bytes = int(result.stdout.split(":")[1].strip())
                        memory_info["total_gb"] = mem_bytes / (1024 ** 3)
                        memory_info["available_gb"] = memory_info["total_gb"] * 0.7  # ä¼°ç®—
            elif platform.system() == "Windows":
                # Windows
                result = subprocess.run(
                    ["wmic", "OS", "get", "TotalVisibleMemorySize", "/value"],
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                if result.returncode == 0:
                    for line in result.stdout.strip().split("\n"):
                        if "=" in line:
                            mem_kb = int(line.split("=")[1])
                            memory_info["total_gb"] = mem_kb / (1024 ** 2)
                            memory_info["available_gb"] = memory_info["total_gb"] * 0.7
        except Exception:
            pass

        return memory_info

    def detect_disk(self) -> Dict[str, Any]:
        """æ£€æµ‹ç£ç›˜ç©ºé—´ä¿¡æ¯"""
        disk_info = {
            "total_gb": 0,
            "available_gb": 0,
        }

        try:
            import shutil
            stat = shutil.disk_usage(Path.cwd())
            disk_info["total_gb"] = stat.total / (1024 ** 3)
            disk_info["available_gb"] = stat.free / (1024 ** 3)
        except Exception:
            pass

        return disk_info

    def detect_all(self) -> Dict[str, Any]:
        """æ£€æµ‹æ‰€æœ‰ä¿¡æ¯"""
        self.info = {
            "os": self.detect_os(),
            "python": self.detect_python(),
            "gpu": self.detect_gpu(),
            "memory": self.detect_memory(),
            "disk": self.detect_disk(),
        }
        return self.info

    def get_recommendations(self) -> Dict[str, Any]:
        """æ ¹æ®æ£€æµ‹ç»“æœæ¨èå­¦ä¹ è·¯å¾„å’Œä¾èµ–å®‰è£…"""
        if not self.info:
            self.detect_all()

        recommendations = {
            "stages": [],
            "dependencies": [],
            "warnings": [],
            "setup_guides": [],
        }

        os_info = self.info["os"]
        python_info = self.info["python"]
        gpu_info = self.info["gpu"]
        memory_info = self.info["memory"]
        disk_info = self.info["disk"]

        # Pythonç‰ˆæœ¬æ£€æŸ¥
        if not python_info["meets_requirement"]:
            recommendations["warnings"].append(
                f"âš ï¸  Pythonç‰ˆæœ¬è¿‡ä½ ({python_info['version']})ï¼Œéœ€è¦Python â‰¥3.9ã€‚"
                f"è¯·å‡çº§Pythonåå†ç»§ç»­ã€‚"
            )
            return recommendations

        if not python_info["recommended"]:
            recommendations["warnings"].append(
                f"âš ï¸  Pythonç‰ˆæœ¬ {python_info['version']} å¯ç”¨ï¼Œä½†æ¨èä½¿ç”¨Python 3.11+ä»¥è·å¾—æ›´å¥½æ€§èƒ½ã€‚"
            )

        # å†…å­˜æ£€æŸ¥
        if memory_info["total_gb"] < 8:
            recommendations["warnings"].append(
                f"âš ï¸  ç³»ç»Ÿå†…å­˜è¾ƒä½ ({memory_info['total_gb']:.1f}GB)ï¼Œæ¨èè‡³å°‘8GBå†…å­˜ã€‚"
                "éƒ¨åˆ†é¡¹ç›®å¯èƒ½è¿è¡Œç¼“æ…¢ã€‚"
            )

        # ç£ç›˜ç©ºé—´æ£€æŸ¥
        if disk_info["available_gb"] < 10:
            recommendations["warnings"].append(
                f"âš ï¸  ç£ç›˜ç©ºé—´ä¸è¶³ ({disk_info['available_gb']:.1f}GBå¯ç”¨)ï¼Œ"
                "æ¨èè‡³å°‘10GBå¯ç”¨ç©ºé—´ã€‚"
            )

        # Stage 3æ¨è (æœºå™¨å­¦ä¹ ï¼ŒCPUå³å¯)
        stage3_rec = {
            "stage": "stage3",
            "name": "æœºå™¨å­¦ä¹ ä¸æ•°æ®æŒ–æ˜",
            "available": True,
            "reason": "âœ… å¯ä»¥åœ¨CPUä¸Šè¿è¡Œï¼Œé€‚åˆæ‰€æœ‰å¹³å°",
            "install_command": 'uv pip install -e ".[stage3]"',
        }
        recommendations["stages"].append(stage3_rec)
        recommendations["dependencies"].append("stage3")

        # Stage 4æ¨è (æ·±åº¦å­¦ä¹ ï¼Œæ¨èGPU)
        stage4_available = memory_info["total_gb"] >= 16 or gpu_info["nvidia_available"] or gpu_info["mps_available"]

        if gpu_info["nvidia_available"]:
            stage4_rec = {
                "stage": "stage4",
                "name": "æ·±åº¦å­¦ä¹ ",
                "available": True,
                "reason": f"âœ… æ£€æµ‹åˆ°NVIDIA GPU ({gpu_info['gpu_names'][0]})ï¼Œæ”¯æŒCUDAåŠ é€Ÿ",
                "install_command": 'uv pip install -e ".[stage4-gpu]"',
                "gpu_type": "CUDA",
            }
            recommendations["dependencies"].append("stage4-gpu")
        elif gpu_info["mps_available"]:
            stage4_rec = {
                "stage": "stage4",
                "name": "æ·±åº¦å­¦ä¹ ",
                "available": True,
                "reason": "âœ… æ£€æµ‹åˆ°Apple SiliconèŠ¯ç‰‡ï¼Œæ”¯æŒMPSåŠ é€Ÿ",
                "install_command": 'uv pip install -e ".[stage4-mps]"',
                "gpu_type": "MPS",
            }
            recommendations["dependencies"].append("stage4-mps")
        elif memory_info["total_gb"] >= 16:
            stage4_rec = {
                "stage": "stage4",
                "name": "æ·±åº¦å­¦ä¹ ",
                "available": True,
                "reason": "âš ï¸  æ— GPUåŠ é€Ÿï¼Œä½¿ç”¨CPUæ¨¡å¼ï¼ˆè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼‰",
                "install_command": 'uv pip install -e ".[stage4-cpu]"',
                "gpu_type": "CPU",
            }
            recommendations["dependencies"].append("stage4-cpu")
            recommendations["warnings"].append(
                "ğŸ’¡ Stage 4æ·±åº¦å­¦ä¹ é¡¹ç›®åœ¨CPUä¸Šè®­ç»ƒè¾ƒæ…¢ï¼Œæ¨èä½¿ç”¨GPUæˆ–äº‘ç«¯ç¯å¢ƒã€‚"
            )
        else:
            stage4_rec = {
                "stage": "stage4",
                "name": "æ·±åº¦å­¦ä¹ ",
                "available": False,
                "reason": "âŒ å†…å­˜ä¸è¶³ä¸”æ— GPUï¼Œä¸æ¨èæœ¬åœ°è¿è¡ŒStage 4",
                "install_command": None,
            }
            recommendations["warnings"].append(
                "âŒ Stage 4æ·±åº¦å­¦ä¹ éœ€è¦16GB+å†…å­˜æˆ–GPUæ”¯æŒã€‚"
                "å»ºè®®ä½¿ç”¨Google Colabæˆ–äº‘ç«¯GPUç¯å¢ƒã€‚"
            )

        recommendations["stages"].append(stage4_rec)

        # Stage 5æ¨è (å¤§æ¨¡å‹ï¼Œå†…å­˜è¦æ±‚é«˜)
        stage5_available = memory_info["total_gb"] >= 16

        if stage5_available:
            stage5_rec = {
                "stage": "stage5",
                "name": "AIGCä¸å¤§æ¨¡å‹",
                "available": True,
                "reason": "âœ… å†…å­˜å……è¶³ï¼Œå¯è¿è¡ŒStage 5ï¼ˆæœ¬åœ°LLMæ¨ç†éœ€32GB+ï¼‰",
                "install_command": 'uv pip install -e ".[stage5]"',
            }
            recommendations["dependencies"].append("stage5")
            if memory_info["total_gb"] < 32:
                recommendations["warnings"].append(
                    "ğŸ’¡ Stage 5ä½¿ç”¨APIè°ƒç”¨æ¨¡å¼ï¼ˆOpenAI/DeepSeekï¼‰ã€‚"
                    "æœ¬åœ°è¿è¡ŒLLMéœ€è¦32GB+å†…å­˜ã€‚"
                )
        else:
            stage5_rec = {
                "stage": "stage5",
                "name": "AIGCä¸å¤§æ¨¡å‹",
                "available": False,
                "reason": "âŒ å†…å­˜ä¸è¶³ (éœ€è¦16GB+)ï¼Œä¸æ¨èæœ¬åœ°è¿è¡Œ",
                "install_command": None,
            }
            recommendations["warnings"].append(
                "âŒ Stage 5å¤§æ¨¡å‹å¼€å‘éœ€è¦16GB+å†…å­˜ã€‚å»ºè®®ä½¿ç”¨äº‘ç«¯ç¯å¢ƒã€‚"
            )

        recommendations["stages"].append(stage5_rec)

        # æ¨èå®‰è£…æŒ‡å¼•
        platform_key = os_info["platform"]
        setup_guide_map = {
            "macOS-Intel": "docs/cross-platform/setup-macos-intel.md",
            "macOS-ARM64": "docs/cross-platform/setup-macos-arm64.md",
            "Linux": "docs/cross-platform/setup-linux.md",
            "WSL2": "docs/cross-platform/setup-windows-wsl2.md",
            "Windows": "docs/cross-platform/setup-windows-native.md",
        }
        recommendations["setup_guides"].append(
            setup_guide_map.get(platform_key, "docs/cross-platform/troubleshooting.md")
        )

        return recommendations

    def print_report(self, show_recommendations: bool = False):
        """æ‰“å°æ£€æµ‹æŠ¥å‘Š"""
        if not self.info:
            self.detect_all()

        print("=" * 60)
        print("ğŸ–¥ï¸  å¹³å°æ£€æµ‹æŠ¥å‘Š (Platform Detection Report)")
        print("=" * 60)
        print()

        # æ“ä½œç³»ç»Ÿ
        os_info = self.info["os"]
        print(f"ğŸ“± æ“ä½œç³»ç»Ÿ: {os_info['platform_name']}")
        print(f"   ç³»ç»Ÿ: {os_info['system']} {os_info['release']}")
        print(f"   æ¶æ„: {os_info['machine']}")
        print()

        # Python
        python_info = self.info["python"]
        status = "âœ…" if python_info["recommended"] else ("âš ï¸ " if python_info["meets_requirement"] else "âŒ")
        print(f"ğŸ Pythonç‰ˆæœ¬: {status} {python_info['version']}")
        print(f"   å®ç°: {python_info['implementation']}")
        print()

        # GPU
        gpu_info = self.info["gpu"]
        if gpu_info["nvidia_available"]:
            print(f"ğŸ® GPU: âœ… NVIDIA CUDA (ç‰ˆæœ¬ {gpu_info['cuda_version']})")
            for i, name in enumerate(gpu_info["gpu_names"]):
                print(f"   GPU {i}: {name}")
        elif gpu_info["mps_available"]:
            print(f"ğŸ® GPU: âœ… Apple MPS (Metal Performance Shaders)")
        else:
            print(f"ğŸ® GPU: âŒ æœªæ£€æµ‹åˆ°GPUåŠ é€Ÿ")
        print()

        # å†…å­˜
        memory_info = self.info["memory"]
        mem_status = "âœ…" if memory_info["total_gb"] >= 16 else ("âš ï¸ " if memory_info["total_gb"] >= 8 else "âŒ")
        print(f"ğŸ’¾ å†…å­˜: {mem_status} {memory_info['total_gb']:.1f}GB æ€»å†…å­˜")
        if memory_info["available_gb"] > 0:
            print(f"   å¯ç”¨: {memory_info['available_gb']:.1f}GB")
        print()

        # ç£ç›˜
        disk_info = self.info["disk"]
        disk_status = "âœ…" if disk_info["available_gb"] >= 20 else ("âš ï¸ " if disk_info["available_gb"] >= 10 else "âŒ")
        print(f"ğŸ’¿ ç£ç›˜ç©ºé—´: {disk_status} {disk_info['available_gb']:.1f}GB å¯ç”¨")
        print(f"   æ€»ç©ºé—´: {disk_info['total_gb']:.1f}GB")
        print()

        # æ¨è
        if show_recommendations:
            print("=" * 60)
            print("ğŸ“‹ æ¨èå­¦ä¹ è·¯å¾„ (Recommended Learning Path)")
            print("=" * 60)
            print()

            recommendations = self.get_recommendations()

            # è­¦å‘Šä¿¡æ¯
            if recommendations["warnings"]:
                print("âš ï¸  æ³¨æ„äº‹é¡¹:")
                for warning in recommendations["warnings"]:
                    print(f"   {warning}")
                print()

            # é˜¶æ®µæ¨è
            print("ğŸ“š å¯ç”¨å­¦ä¹ é˜¶æ®µ:")
            for stage_rec in recommendations["stages"]:
                status = "âœ…" if stage_rec["available"] else "âŒ"
                print(f"   {status} {stage_rec['name']} ({stage_rec['stage']})")
                print(f"      {stage_rec['reason']}")
                if stage_rec["install_command"]:
                    print(f"      å®‰è£…å‘½ä»¤: {stage_rec['install_command']}")
                print()

            # å®‰è£…æŒ‡å¼•
            print("ğŸ“– æ¨èå®‰è£…æŒ‡å¼•:")
            for guide in recommendations["setup_guides"]:
                print(f"   {guide}")
            print()

        print("=" * 60)


def main():
    parser = argparse.ArgumentParser(
        description="æ£€æµ‹å½“å‰è¿è¡Œç¯å¢ƒå¹¶æ¨èå­¦ä¹ è·¯å¾„"
    )
    parser.add_argument(
        "--json",
        action="store_true",
        help="è¾“å‡ºJSONæ ¼å¼ï¼ˆç”¨äºç¨‹åºè°ƒç”¨ï¼‰",
    )
    parser.add_argument(
        "--recommend",
        action="store_true",
        help="æ˜¾ç¤ºæ¨èçš„å­¦ä¹ è·¯å¾„å’Œä¾èµ–å®‰è£…",
    )
    args = parser.parse_args()

    detector = PlatformDetector()
    detector.detect_all()

    if args.json:
        # JSONè¾“å‡º
        output = {
            "detection": detector.info,
            "recommendations": detector.get_recommendations() if args.recommend else None,
        }
        print(json.dumps(output, indent=2, ensure_ascii=False))
    else:
        # äººç±»å¯è¯»è¾“å‡º
        detector.print_report(show_recommendations=args.recommend or True)


if __name__ == "__main__":
    main()
